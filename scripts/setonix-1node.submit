#!/bin/bash

#SBATCH -A pawsey0807-gpu
#SBATCH -J quokka_benchmark
#SBATCH -o 1node_%x-%j.out
#SBATCH -t 00:10:00
#SBATCH -p gpu-dev
#SBATCH --exclusive
#SBATCH --ntasks-per-node=8
#SBATCH --cpus-per-task=8
#SBATCH --sockets-per-node=8
#SBATCH --gpus-per-node=8
#SBATCH -N 1

# load modules
module load craype-accel-amd-gfx90a
module load rocm/5.0.2

# this environment setting is currently needed to work-around a
# known issue with Libfabric
export FI_MR_CACHE_MAX_COUNT=0  # libfabric disable caching

# always run with GPU-aware MPI
export MPICH_GPU_SUPPORT_ENABLED=1

# use correct NIC-to-GPU binding
export MPICH_OFI_NIC_POLICY=NUMA

#----
# create a wrapper to bind 1 GPU per task
wrapper="selectGPU_${SLURM_JOBID}.sh"
cat << EOF > $wrapper
#!/bin/bash
 
export ROCR_VISIBLE_DEVICES=\$SLURM_LOCALID
exec \$*
EOF
chmod +x ./$wrapper
 
#----
# generate an ordered list of CPU-cores (each on a different slurm-socket)
# to be matched with the correct GPU in the srun command using --cpu-bind option.
CPU_BIND="map_cpu:48,56,16,24,0,8,32,40"
 
#----
# run the code
srun -c 8 --cpu-bind=${CPU_BIND} ./$wrapper build/src/HydroBlast3D/test_hydro3d_blast tests/benchmark_unigrid_512.in
 
#----
# delete the wrapper
rm -f ./$wrapper
