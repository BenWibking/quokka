{
  "config": {
    "indexing": "full",
    "lang": [
      "en"
    ],
    "min_search_length": 3,
    "prebuild_index": false,
    "separator": "[\\s\\-]+"
  },
  "docs": [
    {
      "location": "",
      "text": "QUOKKA Quadrilateral, Umbra-producing, Orthogonal, Kangaroo-conserving Kode for Astrophysics! Quokka is a two-moment radiation hydrodynamics code that uses the piecewise-parabolic method, with AMR and subcycling in time. Runs on CPUs (MPI+vectorized) or NVIDIA GPUs (MPI+CUDA) with a single-source codebase. Written in C++17. (100% Fortran-free.) Note The Quokka methods paper is now available on arXiv . We use the AMReX library (Zhang et al., 2019) 1 to provide patch-based adaptive mesh functionality. We take advantage of the C++ loop abstractions in AMReX in order to run with high performance on either CPUs or GPUs. Example simulation set-ups are included in the GitHub repository for many astrophysical problems of interest related to star formation and the interstellar medium. Contact All communication takes place on the Quokka GitHub repository . You can start a Discussion for technical support, or open an Issue for any bug reports. Zhang, W., Almgren, A., Beckner, V., Bell, J., Blaschke, J., Chan, C., \u2026 Zingale, M. (2019). AMReX: A framework for block-structured adaptive mesh refinement. Journal of Open Source Software , 4 (37), 1370. https://doi.org/10.21105/joss.01370 \u21a9",
      "title": "QUOKKA"
    },
    {
      "location": "#quokka",
      "text": "Quadrilateral, Umbra-producing, Orthogonal, Kangaroo-conserving Kode for Astrophysics! Quokka is a two-moment radiation hydrodynamics code that uses the piecewise-parabolic method, with AMR and subcycling in time. Runs on CPUs (MPI+vectorized) or NVIDIA GPUs (MPI+CUDA) with a single-source codebase. Written in C++17. (100% Fortran-free.) Note The Quokka methods paper is now available on arXiv . We use the AMReX library (Zhang et al., 2019) 1 to provide patch-based adaptive mesh functionality. We take advantage of the C++ loop abstractions in AMReX in order to run with high performance on either CPUs or GPUs. Example simulation set-ups are included in the GitHub repository for many astrophysical problems of interest related to star formation and the interstellar medium.",
      "title": "QUOKKA"
    },
    {
      "location": "#contact",
      "text": "All communication takes place on the Quokka GitHub repository . You can start a Discussion for technical support, or open an Issue for any bug reports. Zhang, W., Almgren, A., Beckner, V., Bell, J., Blaschke, J., Chan, C., \u2026 Zingale, M. (2019). AMReX: A framework for block-structured adaptive mesh refinement. Journal of Open Source Software , 4 (37), 1370. https://doi.org/10.21105/joss.01370 \u21a9",
      "title": "Contact"
    },
    {
      "location": "about/",
      "text": "About Quokka is a high-resolution shock capturing AMR radiation hydrodynamics code using the AMReX library (Zhang et al., 2019) 1 to provide patch-based adaptive mesh functionality. We take advantage of the C++ loop abstractions in AMReX in order to run with high performance on either CPUs, NVIDIA GPUs, or AMD GPUs. Development methodology The code is written in modern C++17, using MPI for distributed-memory parallelism, with the AMReX GPU abstraction compiling as either native CUDA code or native HIP code when GPU support is enabled. We use a modern C++ development methodology, using CMake, CTest, and Doxygen. We use clang-format for automated code formatting, and clang-tidy and SonarCloud for static analysis, in order to audit code adherence to the ISO C++ Core Guidelines and the MISRA C/C++ guidelines. We additionally ensure the code is free of memory corruption bugs using Clang's AddressSanitizer . There is an automated suite of test problems that can be run using CTest. Each test problem has a validated solution against which it is compared (usually in L1 norm) in order to pass. Code development is managed using pull requests (PRs) on GitHub. In an effort to ensure long-term code maintainability, all code must be written in C++17 following the Coding Guidelines, it must compile using Clang without warnings, all tests must pass, and the static analyzers must show zero new bugs before a pull request is merged with the main branch. User assistance and bug reports are managed via Discussions and Issues in the GitHub repository. Numerical methods Hydrodynamics The hydrodynamics solver is an unsplit method, using the piecewise parabolic method (Colella & Woodward, 1984) 2 for reconstruction in the primitive variables, the HLLC Riemann solver (Toro, 2013) 3 for flux computations, and a method-of-lines formulation for the time integration. We use the method of (Miller & Colella, 2002) 4 to reduce the order of reconstruction in zones where shocks are detected in order to suppress spurious oscillations in strong shocks. Radiation The radiation hydrodynamics formulation is based on the mixed-frame moment equations (e.g., (Mihalas & Mihalas, 1984) 5 ). The radiation subsystem is coupled to the hydrodynamic subsystem via operator splitting, with the hydrodynamic update computed first, followed by the radiation update, with the latter update including the source terms corresponding to the radiation four-force applied to both the radiation and hydrodynamic variables. A method-of-lines formulation is also used for the time integration, with the time integration done by the same integrator chosen for the hydrodynamic subsystem. The hyperbolic radiation subsystem is solved using an unsplit method, using PPM for reconstruction of the moment variables, with fluxes computed via the HLL Riemann solver, with the wavespeeds computed using the 'frozen Eddington factor' approximation (Balsara, 1999) 6 , which is more robust than using the eigenvalues of the M1 system (Skinner & Ostriker, 2013) 7 itself. We reconstruct the energy density and the reduced flux \\(f = F/cE\\) , in order to maintain the flux-limiting condition \\(F \\le cE\\) in discontinuous and near-discontinuous radiation flows. To ensure the correct behavior of the advection terms in the asymptotic diffusion limit (Lowrie & Morel, 2001) 8 , we modify the Riemann solver according to (Skinner, Dolence, Burrows, Radice, & Vartanyan, 2019) 9 . We use the Lorentz-factor local closure of (Levermore, 1984) 10 to compute the variable Eddington tensor. The source terms corresponding to matter-radiation energy exchange are solved implicitly with the method of (Howell & Greenough, 2003) 11 following the hyperbolic subsystem update. The matter-radiation momentum update is likewise computed implicitly in order to maintain the correct behavior in the asymptotic diffusion limit (Skinner, Dolence, Burrows, Radice, & Vartanyan, 2019) 9 . Zhang, W., Almgren, A., Beckner, V., Bell, J., Blaschke, J., Chan, C., \u2026 Zingale, M. (2019). AMReX: A framework for block-structured adaptive mesh refinement. Journal of Open Source Software , 4 (37), 1370. https://doi.org/10.21105/joss.01370 \u21a9 Colella, P., & Woodward, P. R. (1984). The Piecewise Parabolic Method (PPM) for Gas-Dynamical Simulations . Journal of Computational Physics , 54 , 174\u2013201. https://doi.org/10.1016/0021-9991(84)90143-8 \u21a9 Toro, E. F. (2013). Riemann solvers and numerical methods for fluid dynamics: A practical introduction . Springer Berlin Heidelberg. Retrieved from https://books.google.com.au/books?id=zkLtCAAAQBAJ \u21a9 Miller, G. H., & Colella, P. (2002). A Conservative Three-Dimensional Eulerian Method for Coupled Solid-Fluid Shock Capturing . 183 (1), 26\u201382. https://doi.org/10.1006/jcph.2002.7158 \u21a9 Mihalas, D., & Mihalas, B. W. (1984). Foundations of radiation hydrodynamics . Oxford University Press. \u21a9 Balsara, D. S. (1999). An analysis of the hyperbolic nature of the equations of radiation hydrodynamics. 61 (5), 617\u2013627. https://doi.org/10.1016/S0022-4073(98)00049-1 \u21a9 Skinner, M. A., & Ostriker, E. C. (2013). A Two-moment Radiation Hydrodynamics Module in Athena Using a Time-explicit Godunov Method . 206 (2), 21. https://doi.org/10.1088/0067-0049/206/2/21 \u21a9 Lowrie, R. B., & Morel, J. E. (2001). Issues with high-resolution Godunov methods for radiation hydrodynamics . 69 , 475\u2013489. https://doi.org/10.1016/S0022-4073(00)00097-2 \u21a9 Skinner, M. A., Dolence, J. C., Burrows, A., Radice, D., & Vartanyan, D. (2019). FORNAX: A Flexible Code for Multiphysics Astrophysical Simulations . 241 (1), 7. https://doi.org/10.3847/1538-4365/ab007f \u21a9 \u21a9 Levermore, C. D. (1984). Relating Eddington factors to flux limiters. 31 (2), 149\u2013160. https://doi.org/10.1016/0022-4073(84)90112-2 \u21a9 Howell, L. H., & Greenough, J. A. (2003). Radiation diffusion for multi-fluid Eulerian hydrodynamics with adaptive mesh refinement . Journal of Computational Physics , 184 (1), 53\u201378. https://doi.org/10.1016/S0021-9991(02)00015-3 \u21a9",
      "title": "About"
    },
    {
      "location": "about/#about",
      "text": "Quokka is a high-resolution shock capturing AMR radiation hydrodynamics code using the AMReX library (Zhang et al., 2019) 1 to provide patch-based adaptive mesh functionality. We take advantage of the C++ loop abstractions in AMReX in order to run with high performance on either CPUs, NVIDIA GPUs, or AMD GPUs.",
      "title": "About"
    },
    {
      "location": "about/#development-methodology",
      "text": "The code is written in modern C++17, using MPI for distributed-memory parallelism, with the AMReX GPU abstraction compiling as either native CUDA code or native HIP code when GPU support is enabled. We use a modern C++ development methodology, using CMake, CTest, and Doxygen. We use clang-format for automated code formatting, and clang-tidy and SonarCloud for static analysis, in order to audit code adherence to the ISO C++ Core Guidelines and the MISRA C/C++ guidelines. We additionally ensure the code is free of memory corruption bugs using Clang's AddressSanitizer . There is an automated suite of test problems that can be run using CTest. Each test problem has a validated solution against which it is compared (usually in L1 norm) in order to pass. Code development is managed using pull requests (PRs) on GitHub. In an effort to ensure long-term code maintainability, all code must be written in C++17 following the Coding Guidelines, it must compile using Clang without warnings, all tests must pass, and the static analyzers must show zero new bugs before a pull request is merged with the main branch. User assistance and bug reports are managed via Discussions and Issues in the GitHub repository.",
      "title": "Development methodology"
    },
    {
      "location": "about/#numerical-methods",
      "text": "",
      "title": "Numerical methods"
    },
    {
      "location": "about/#hydrodynamics",
      "text": "The hydrodynamics solver is an unsplit method, using the piecewise parabolic method (Colella & Woodward, 1984) 2 for reconstruction in the primitive variables, the HLLC Riemann solver (Toro, 2013) 3 for flux computations, and a method-of-lines formulation for the time integration. We use the method of (Miller & Colella, 2002) 4 to reduce the order of reconstruction in zones where shocks are detected in order to suppress spurious oscillations in strong shocks.",
      "title": "Hydrodynamics"
    },
    {
      "location": "about/#radiation",
      "text": "The radiation hydrodynamics formulation is based on the mixed-frame moment equations (e.g., (Mihalas & Mihalas, 1984) 5 ). The radiation subsystem is coupled to the hydrodynamic subsystem via operator splitting, with the hydrodynamic update computed first, followed by the radiation update, with the latter update including the source terms corresponding to the radiation four-force applied to both the radiation and hydrodynamic variables. A method-of-lines formulation is also used for the time integration, with the time integration done by the same integrator chosen for the hydrodynamic subsystem. The hyperbolic radiation subsystem is solved using an unsplit method, using PPM for reconstruction of the moment variables, with fluxes computed via the HLL Riemann solver, with the wavespeeds computed using the 'frozen Eddington factor' approximation (Balsara, 1999) 6 , which is more robust than using the eigenvalues of the M1 system (Skinner & Ostriker, 2013) 7 itself. We reconstruct the energy density and the reduced flux \\(f = F/cE\\) , in order to maintain the flux-limiting condition \\(F \\le cE\\) in discontinuous and near-discontinuous radiation flows. To ensure the correct behavior of the advection terms in the asymptotic diffusion limit (Lowrie & Morel, 2001) 8 , we modify the Riemann solver according to (Skinner, Dolence, Burrows, Radice, & Vartanyan, 2019) 9 . We use the Lorentz-factor local closure of (Levermore, 1984) 10 to compute the variable Eddington tensor. The source terms corresponding to matter-radiation energy exchange are solved implicitly with the method of (Howell & Greenough, 2003) 11 following the hyperbolic subsystem update. The matter-radiation momentum update is likewise computed implicitly in order to maintain the correct behavior in the asymptotic diffusion limit (Skinner, Dolence, Burrows, Radice, & Vartanyan, 2019) 9 . Zhang, W., Almgren, A., Beckner, V., Bell, J., Blaschke, J., Chan, C., \u2026 Zingale, M. (2019). AMReX: A framework for block-structured adaptive mesh refinement. Journal of Open Source Software , 4 (37), 1370. https://doi.org/10.21105/joss.01370 \u21a9 Colella, P., & Woodward, P. R. (1984). The Piecewise Parabolic Method (PPM) for Gas-Dynamical Simulations . Journal of Computational Physics , 54 , 174\u2013201. https://doi.org/10.1016/0021-9991(84)90143-8 \u21a9 Toro, E. F. (2013). Riemann solvers and numerical methods for fluid dynamics: A practical introduction . Springer Berlin Heidelberg. Retrieved from https://books.google.com.au/books?id=zkLtCAAAQBAJ \u21a9 Miller, G. H., & Colella, P. (2002). A Conservative Three-Dimensional Eulerian Method for Coupled Solid-Fluid Shock Capturing . 183 (1), 26\u201382. https://doi.org/10.1006/jcph.2002.7158 \u21a9 Mihalas, D., & Mihalas, B. W. (1984). Foundations of radiation hydrodynamics . Oxford University Press. \u21a9 Balsara, D. S. (1999). An analysis of the hyperbolic nature of the equations of radiation hydrodynamics. 61 (5), 617\u2013627. https://doi.org/10.1016/S0022-4073(98)00049-1 \u21a9 Skinner, M. A., & Ostriker, E. C. (2013). A Two-moment Radiation Hydrodynamics Module in Athena Using a Time-explicit Godunov Method . 206 (2), 21. https://doi.org/10.1088/0067-0049/206/2/21 \u21a9 Lowrie, R. B., & Morel, J. E. (2001). Issues with high-resolution Godunov methods for radiation hydrodynamics . 69 , 475\u2013489. https://doi.org/10.1016/S0022-4073(00)00097-2 \u21a9 Skinner, M. A., Dolence, J. C., Burrows, A., Radice, D., & Vartanyan, D. (2019). FORNAX: A Flexible Code for Multiphysics Astrophysical Simulations . 241 (1), 7. https://doi.org/10.3847/1538-4365/ab007f \u21a9 \u21a9 Levermore, C. D. (1984). Relating Eddington factors to flux limiters. 31 (2), 149\u2013160. https://doi.org/10.1016/0022-4073(84)90112-2 \u21a9 Howell, L. H., & Greenough, J. A. (2003). Radiation diffusion for multi-fluid Eulerian hydrodynamics with adaptive mesh refinement . Journal of Computational Physics , 184 (1), 53\u201378. https://doi.org/10.1016/S0021-9991(02)00015-3 \u21a9",
      "title": "Radiation"
    },
    {
      "location": "citation/",
      "text": "Citation If you use Quokka or the numerical methods originally implemented for Quokka in your research, please cite (Wibking & Krumholz, 2022) 1 @ARTICLE { 2022MNRAS.512.1430W , author = {{Wibking}, Benjamin D. and {Krumholz}, Mark R.} , title = \"{QUOKKA: a code for two-moment AMR radiation hydrodynamics on GPUs}\" , journal = {\\mnras} , keywords = {hydrodynamics, methods: numerical, Astrophysics - Instrumentation and Methods for Astrophysics} , year = 2022 , month = may , volume = {512} , number = {1} , pages = {1430-1449} , doi = {10.1093/mnras/stac439} , archivePrefix = {arXiv} , eprint = {2110.01792} , primaryClass = {astro-ph.IM} , adsurl = {https://ui.adsabs.harvard.edu/abs/2022MNRAS.512.1430W} , adsnote = {Provided by the SAO/NASA Astrophysics Data System} Additional methods papers If you use any of the following numerical methods or physical modules, please also cite the publications corresponding to the methods used in your research. Radiation-hydrodynamics: (He, Wibking, & Krumholz, 2024) 2 Multigroup Radiation-hydrodynamics: (He, Wibking, & Krumholz, 2024) 3 Scientific applications with Quokka Galactic outflows: (Vijayan, Krumholz, & Wibking, 2024) 4 Wibking, B. D., & Krumholz, M. R. (2022). QUOKKA: a code for two-moment AMR radiation hydrodynamics on GPUs . 512 (1), 1430\u20131449. https://doi.org/10.1093/mnras/stac439 \u21a9 He, C.-C., Wibking, B. D., & Krumholz, M. R. (2024). An asymptotically correct implicit-explicit time integration scheme for finite volume radiation-hydrodynamics . 531 (1), 1228\u20131242. https://doi.org/10.1093/mnras/stae1244 \u21a9 He, C.-C., Wibking, B. D., & Krumholz, M. R. (2024). A novel numerical method for mixed-frame multigroup radiation-hydrodynamics with GPU acceleration implemented in the QUOKKA code . arXiv e-Prints , arXiv:2407.18304. Retrieved from https://arxiv.org/abs/2407.18304 \u21a9 Vijayan, A., Krumholz, M. R., & Wibking, B. D. (2024). QUOKKA-based understanding of outflows (QED) - I. Metal loading, phase structure, and convergence testing for solar neighbourhood conditions . 527 (4), 10095\u201310110. https://doi.org/10.1093/mnras/stad3816 \u21a9",
      "title": "Citation"
    },
    {
      "location": "citation/#citation",
      "text": "If you use Quokka or the numerical methods originally implemented for Quokka in your research, please cite (Wibking & Krumholz, 2022) 1 @ARTICLE { 2022MNRAS.512.1430W , author = {{Wibking}, Benjamin D. and {Krumholz}, Mark R.} , title = \"{QUOKKA: a code for two-moment AMR radiation hydrodynamics on GPUs}\" , journal = {\\mnras} , keywords = {hydrodynamics, methods: numerical, Astrophysics - Instrumentation and Methods for Astrophysics} , year = 2022 , month = may , volume = {512} , number = {1} , pages = {1430-1449} , doi = {10.1093/mnras/stac439} , archivePrefix = {arXiv} , eprint = {2110.01792} , primaryClass = {astro-ph.IM} , adsurl = {https://ui.adsabs.harvard.edu/abs/2022MNRAS.512.1430W} , adsnote = {Provided by the SAO/NASA Astrophysics Data System}",
      "title": "Citation"
    },
    {
      "location": "citation/#additional-methods-papers",
      "text": "If you use any of the following numerical methods or physical modules, please also cite the publications corresponding to the methods used in your research. Radiation-hydrodynamics: (He, Wibking, & Krumholz, 2024) 2 Multigroup Radiation-hydrodynamics: (He, Wibking, & Krumholz, 2024) 3",
      "title": "Additional methods papers"
    },
    {
      "location": "citation/#scientific-applications-with-quokka",
      "text": "Galactic outflows: (Vijayan, Krumholz, & Wibking, 2024) 4 Wibking, B. D., & Krumholz, M. R. (2022). QUOKKA: a code for two-moment AMR radiation hydrodynamics on GPUs . 512 (1), 1430\u20131449. https://doi.org/10.1093/mnras/stac439 \u21a9 He, C.-C., Wibking, B. D., & Krumholz, M. R. (2024). An asymptotically correct implicit-explicit time integration scheme for finite volume radiation-hydrodynamics . 531 (1), 1228\u20131242. https://doi.org/10.1093/mnras/stae1244 \u21a9 He, C.-C., Wibking, B. D., & Krumholz, M. R. (2024). A novel numerical method for mixed-frame multigroup radiation-hydrodynamics with GPU acceleration implemented in the QUOKKA code . arXiv e-Prints , arXiv:2407.18304. Retrieved from https://arxiv.org/abs/2407.18304 \u21a9 Vijayan, A., Krumholz, M. R., & Wibking, B. D. (2024). QUOKKA-based understanding of outflows (QED) - I. Metal loading, phase structure, and convergence testing for solar neighbourhood conditions . 527 (4), 10095\u201310110. https://doi.org/10.1093/mnras/stad3816 \u21a9",
      "title": "Scientific applications with Quokka"
    },
    {
      "location": "debugging/",
      "text": "Debugging General guidelines A good general guide to debugging simulation codes is provided by WarpX (although some details are WarpX-specific). Common bugs You might be reading this page because you have encountered a common type of bug: an out-of-bounds array access This is when the code accesses an array with an index that corresponds to an element that doesn't actually exist. In C++, this causes the computer to access a memory location that is completely unrelated to the array that you intended to access. In a CPU code, this usually causes a silently incorrect result, but on GPU, this may actually cause the simulation to crash. However, if you are accessing an amrex::Array4 object and you have compiled in Debug mode , then AMReX will issue an error message when this occurs. There is a significant performance cost to this error checking, so it does not occur when compiled in Release mode. accessing a host variable from the GPU The second most common type of bug encountered in Quokka is accessing a host variable (i.e., a variable that can only be accessed from code that runs on the CPU) from code running on the GPU (i.e., within a ParallelFor ). Sometimes the compiler will detect this situation and print an error message, but often this will only present an issue when actuallly running the code -- for instance, this can happen when the GPU code tries to dereference a pointer to an address in CPU memory. In that case, the only way to debug this error is to run Quokka under cuda-gdb (or, on AMD GPUs, rocgdb ). How to debug on GPUs The best way to debug on GPUs is to... not debug on GPUs. That is, it is always easier to instead debug the problem on a CPU-only run. GPU debugging is very painful and itself quite buggy. This is unfortunately true for all GPU vendors. Try to shrink the problem to a size that can run on a single node, or (even better) a single MPI rank / CPU core, but where it still exhibits the error that you are trying to debug. Build Quokka without GPU support but with -DCMAKE_BUILD_TYPE=Debug and re-run. If there are any array out-of-bounds errors, it will stop and report exactly which array is being accessed out-of-bounds and what the indices are. The only downside is that Quokka will run very slowly in this mode. For more details, see the AMReX debugging guide . Build Quokka without GPU support but with -DCMAKE_BUILD_TYPE=Release -DENABLE_ASAN=ON . This turns on the AddressSanitizer, which checks for out-of-bounds array accesses and other memory bugs. This is faster than the previous method, but it produces less informative error messages (e.g., no array indices). This method may produce a lot of messages about memory leaks, which are not necessarily bugs , and should not cause GPU crashes. These messages can be disabled if you are looking for, e.g., out-of-bounds array accesses, which is a class of bug that can cause a GPU crash. It is recommended to set these environment variables when you run it: ASAN_OPTIONS=abort_on_error=1:fast_unwind_on_malloc=1:detect_leaks=0 UBSAN_OPTIONS=print_stacktrace=0 . Note that CTest appends its own options to this environment variable when running tests, so it is recommended to run the simulation manually (i.e., without make test , ninja test , or ctest ). For more information, see this guide to using AddressSanitizer in an HPC context . On AMD GPUs, there is a GPU-aware AddressSanitizer . Currently, enabling this requires manually changing the compiler flags. How to actually debug on GPUs As an absolute last resort if it is impossible to reproduce the error you are seeing on a CPU-only run, then the best option is to: downsize the simulation to fit on a single GPU start the simulation on an NVIDIA GPU from within CUDA-GDB (see the CUDA-GDB documentation and slides ). hope CUDA-GDB does not itself crash hope CUDA-GDB produces a useful error message that you can analyze NVIDIA also provides the compute-sanitizer tool that is essentially the equivalent of AddressSanitizer (see the ComputeSanitizer documentation ). Unfortunately, it does not work as reliably as AddressSanitizer, and may itself crash while attempting to debug a GPU program. For AMD GPUs, you have to use the AMD-provided debugger rocgdb . A tutorial its use is available here . AMD also provides a GPU-aware AddressSanitizer that can be enabled when building Quokka. Currently, the compiler flags must be manually modified in order to enable this. For details, see its documentation . GPU kernel asynchronicity By default, GPU kernels launch asynchronously, i.e., execution of CPU code continues before the kernel starts on the GPU. This can cause synchronization problems if there is an implicit assumption about the order of operations with respect to CPU and GPU code. The easiest way to debug this is to set the environment variables: CUDA_LAUNCH_BLOCKING=1 on NVIDIA GPUs, or HIP_LAUNCH_BLOCKING=1 on AMD GPUs. This will cause the CPU to wait until the GPU kernel execution is complete before continuing past the call to ParallelFor . For more details, refer to the AMReX GPU debugging guide . When all else fails: Debugging with printf If you have tried all of the above steps, then you have to resort to adding printf statements within the GPU code. Note that printf inside GPU code is different from the CPU-side printf function, as explained in the NVIDIA documentation .",
      "title": "Debugging"
    },
    {
      "location": "debugging/#debugging",
      "text": "",
      "title": "Debugging"
    },
    {
      "location": "debugging/#general-guidelines",
      "text": "A good general guide to debugging simulation codes is provided by WarpX (although some details are WarpX-specific).",
      "title": "General guidelines"
    },
    {
      "location": "debugging/#common-bugs",
      "text": "You might be reading this page because you have encountered a common type of bug: an out-of-bounds array access This is when the code accesses an array with an index that corresponds to an element that doesn't actually exist. In C++, this causes the computer to access a memory location that is completely unrelated to the array that you intended to access. In a CPU code, this usually causes a silently incorrect result, but on GPU, this may actually cause the simulation to crash. However, if you are accessing an amrex::Array4 object and you have compiled in Debug mode , then AMReX will issue an error message when this occurs. There is a significant performance cost to this error checking, so it does not occur when compiled in Release mode. accessing a host variable from the GPU The second most common type of bug encountered in Quokka is accessing a host variable (i.e., a variable that can only be accessed from code that runs on the CPU) from code running on the GPU (i.e., within a ParallelFor ). Sometimes the compiler will detect this situation and print an error message, but often this will only present an issue when actuallly running the code -- for instance, this can happen when the GPU code tries to dereference a pointer to an address in CPU memory. In that case, the only way to debug this error is to run Quokka under cuda-gdb (or, on AMD GPUs, rocgdb ).",
      "title": "Common bugs"
    },
    {
      "location": "debugging/#how-to-debug-on-gpus",
      "text": "The best way to debug on GPUs is to... not debug on GPUs. That is, it is always easier to instead debug the problem on a CPU-only run. GPU debugging is very painful and itself quite buggy. This is unfortunately true for all GPU vendors. Try to shrink the problem to a size that can run on a single node, or (even better) a single MPI rank / CPU core, but where it still exhibits the error that you are trying to debug. Build Quokka without GPU support but with -DCMAKE_BUILD_TYPE=Debug and re-run. If there are any array out-of-bounds errors, it will stop and report exactly which array is being accessed out-of-bounds and what the indices are. The only downside is that Quokka will run very slowly in this mode. For more details, see the AMReX debugging guide . Build Quokka without GPU support but with -DCMAKE_BUILD_TYPE=Release -DENABLE_ASAN=ON . This turns on the AddressSanitizer, which checks for out-of-bounds array accesses and other memory bugs. This is faster than the previous method, but it produces less informative error messages (e.g., no array indices). This method may produce a lot of messages about memory leaks, which are not necessarily bugs , and should not cause GPU crashes. These messages can be disabled if you are looking for, e.g., out-of-bounds array accesses, which is a class of bug that can cause a GPU crash. It is recommended to set these environment variables when you run it: ASAN_OPTIONS=abort_on_error=1:fast_unwind_on_malloc=1:detect_leaks=0 UBSAN_OPTIONS=print_stacktrace=0 . Note that CTest appends its own options to this environment variable when running tests, so it is recommended to run the simulation manually (i.e., without make test , ninja test , or ctest ). For more information, see this guide to using AddressSanitizer in an HPC context . On AMD GPUs, there is a GPU-aware AddressSanitizer . Currently, enabling this requires manually changing the compiler flags.",
      "title": "How to debug on GPUs"
    },
    {
      "location": "debugging/#how-to-actually-debug-on-gpus",
      "text": "As an absolute last resort if it is impossible to reproduce the error you are seeing on a CPU-only run, then the best option is to: downsize the simulation to fit on a single GPU start the simulation on an NVIDIA GPU from within CUDA-GDB (see the CUDA-GDB documentation and slides ). hope CUDA-GDB does not itself crash hope CUDA-GDB produces a useful error message that you can analyze NVIDIA also provides the compute-sanitizer tool that is essentially the equivalent of AddressSanitizer (see the ComputeSanitizer documentation ). Unfortunately, it does not work as reliably as AddressSanitizer, and may itself crash while attempting to debug a GPU program. For AMD GPUs, you have to use the AMD-provided debugger rocgdb . A tutorial its use is available here . AMD also provides a GPU-aware AddressSanitizer that can be enabled when building Quokka. Currently, the compiler flags must be manually modified in order to enable this. For details, see its documentation .",
      "title": "How to actually debug on GPUs"
    },
    {
      "location": "debugging/#gpu-kernel-asynchronicity",
      "text": "By default, GPU kernels launch asynchronously, i.e., execution of CPU code continues before the kernel starts on the GPU. This can cause synchronization problems if there is an implicit assumption about the order of operations with respect to CPU and GPU code. The easiest way to debug this is to set the environment variables: CUDA_LAUNCH_BLOCKING=1 on NVIDIA GPUs, or HIP_LAUNCH_BLOCKING=1 on AMD GPUs. This will cause the CPU to wait until the GPU kernel execution is complete before continuing past the call to ParallelFor . For more details, refer to the AMReX GPU debugging guide .",
      "title": "GPU kernel asynchronicity"
    },
    {
      "location": "debugging/#when-all-else-fails-debugging-with-printf",
      "text": "If you have tried all of the above steps, then you have to resort to adding printf statements within the GPU code. Note that printf inside GPU code is different from the CPU-side printf function, as explained in the NVIDIA documentation .",
      "title": "When all else fails: Debugging with printf"
    },
    {
      "location": "equations/",
      "text": "Equations Fluids and radiation Assuming the speed of light is not reduced ( \\(\\hat{c} = c\\) ), Quokka solves the system of conservation laws: \\[\\frac{\\partial \\vec{U}}{\\partial t}+\\nabla \\cdot \\vec{F}(\\vec{U}) = \\vec{S}(\\vec{U}),\\] \\[\\begin{aligned} \\vec{U} =\\left[ \\begin{array}{c} \\rho \\\\ \\rho \\vec{v} \\\\ E_{\\rm gas} \\\\ \\rho X_n \\\\ E_g \\\\ \\vec{F}_g \\end{array}\\right], \\; \\vec{F}(U) = \\left[ \\begin{array}{c} \\rho \\vec{v} \\\\ \\rho \\vec{v} \\otimes \\vec{v}+p \\\\ (E_{\\rm gas} + p) \\vec{v} \\\\ \\rho X_n \\vec{v} \\\\ \\vec{F}_g \\\\ c^2 \\mathsf{P}_g \\end{array}\\right], \\; \\vec{S}(U)=\\left[ \\begin{array}{c} 0 \\\\ \\sum_g \\vec{G}_g + \\rho \\vec{g} \\\\ c \\sum_g G^0_{g} + \\rho \\vec{v} \\cdot \\vec{g} + \\mathcal{H} - \\mathcal{C} \\\\ \\rho \\dot{X}_n \\\\ - c G^0_{g} \\\\ - c^2 \\vec{G}_g \\end{array}\\right], \\end{aligned}\\] along with the non-conservative auxiliary internal energy equation: \\[\\begin{aligned} \\frac{\\partial (\\rho e_{\\text{aux}})}{\\partial t} = - \\nabla \\cdot (\\rho e_{\\text{aux}} \\vec{v}) - p \\nabla \\cdot \\vec{v} + S_{\\text{rad}} + \\mathcal{H} - \\mathcal{C}, \\\\ \\Delta S_{\\text{rad}} = \\int \\sum_g c G^0_g \\ dt - \\frac{1}{2} \\Delta \\left(\\rho v^2 \\right), \\end{aligned}\\] and the gravitational Poisson equation: \\[\\begin{aligned} \\nabla^2 \\phi = -4 \\pi G \\left( \\rho + \\sum_i \\rho_i \\right), \\\\ \\vec{g} \\equiv -\\nabla \\phi, \\end{aligned}\\] where \\(\\rho\\) is the gas density, \\(\\vec{v}\\) is the gas velocity, \\(E_{\\text{gas}}\\) is the total gas energy, \\(\\rho e_{\\text{aux}}\\) is the auxiliary gas internal energy, \\(X_n\\) is the fractional concentration of species \\(n\\) , \\(\\dot{X}_n\\) is the chemical reaction term for species \\(n\\) , \\(\\mathcal{H}\\) is the optically-thin volumetric heating term (radiative and chemical), \\(\\mathcal{C}\\) is the optically-thin volumetric cooling term (radiative and chemical), \\(p(\\rho, e)\\) is the gas pressure derived from a general convex equation of state, \\(E_g\\) is the radiation energy density for group \\(g\\) , \\(F_g\\) is the radiation flux for group \\(g\\) , \\(\\mathsf{P}_g\\) is the radiation pressure tensor for group \\(g\\) , \\(G_g\\) is the radiation four-force \\([G^0_g, \\vec{G}_g]\\) due to group \\(g\\) , \\(\\Delta S_{\\text{rad}}\\) is the change in gas internal energy due to radiation over a timestep, \\(\\phi\\) is the Newtonian gravitational potential, \\(\\vec{g}\\) is the gravitational acceleration, \\(\\rho_i\\) is the mass density due to particle \\(i\\) . Note that since work done by radiation on the gas is included in the \\(c \\sum_g G^0_g\\) term, \\(S_{\\text{rad}}\\) is not the same as \\(c \\sum_g G^0_g\\) . Collisionless particles Quokka solves the following equation of motion for collisionless particles: \\[\\frac{d^2 \\vec{x}_i}{d t^2} = \\vec{g} ,\\] where \\(\\vec{x}_i\\) is the position vector of particle \\(i\\) .",
      "title": "Equations"
    },
    {
      "location": "equations/#equations",
      "text": "",
      "title": "Equations"
    },
    {
      "location": "equations/#fluids-and-radiation",
      "text": "Assuming the speed of light is not reduced ( \\(\\hat{c} = c\\) ), Quokka solves the system of conservation laws: \\[\\frac{\\partial \\vec{U}}{\\partial t}+\\nabla \\cdot \\vec{F}(\\vec{U}) = \\vec{S}(\\vec{U}),\\] \\[\\begin{aligned} \\vec{U} =\\left[ \\begin{array}{c} \\rho \\\\ \\rho \\vec{v} \\\\ E_{\\rm gas} \\\\ \\rho X_n \\\\ E_g \\\\ \\vec{F}_g \\end{array}\\right], \\; \\vec{F}(U) = \\left[ \\begin{array}{c} \\rho \\vec{v} \\\\ \\rho \\vec{v} \\otimes \\vec{v}+p \\\\ (E_{\\rm gas} + p) \\vec{v} \\\\ \\rho X_n \\vec{v} \\\\ \\vec{F}_g \\\\ c^2 \\mathsf{P}_g \\end{array}\\right], \\; \\vec{S}(U)=\\left[ \\begin{array}{c} 0 \\\\ \\sum_g \\vec{G}_g + \\rho \\vec{g} \\\\ c \\sum_g G^0_{g} + \\rho \\vec{v} \\cdot \\vec{g} + \\mathcal{H} - \\mathcal{C} \\\\ \\rho \\dot{X}_n \\\\ - c G^0_{g} \\\\ - c^2 \\vec{G}_g \\end{array}\\right], \\end{aligned}\\] along with the non-conservative auxiliary internal energy equation: \\[\\begin{aligned} \\frac{\\partial (\\rho e_{\\text{aux}})}{\\partial t} = - \\nabla \\cdot (\\rho e_{\\text{aux}} \\vec{v}) - p \\nabla \\cdot \\vec{v} + S_{\\text{rad}} + \\mathcal{H} - \\mathcal{C}, \\\\ \\Delta S_{\\text{rad}} = \\int \\sum_g c G^0_g \\ dt - \\frac{1}{2} \\Delta \\left(\\rho v^2 \\right), \\end{aligned}\\] and the gravitational Poisson equation: \\[\\begin{aligned} \\nabla^2 \\phi = -4 \\pi G \\left( \\rho + \\sum_i \\rho_i \\right), \\\\ \\vec{g} \\equiv -\\nabla \\phi, \\end{aligned}\\] where \\(\\rho\\) is the gas density, \\(\\vec{v}\\) is the gas velocity, \\(E_{\\text{gas}}\\) is the total gas energy, \\(\\rho e_{\\text{aux}}\\) is the auxiliary gas internal energy, \\(X_n\\) is the fractional concentration of species \\(n\\) , \\(\\dot{X}_n\\) is the chemical reaction term for species \\(n\\) , \\(\\mathcal{H}\\) is the optically-thin volumetric heating term (radiative and chemical), \\(\\mathcal{C}\\) is the optically-thin volumetric cooling term (radiative and chemical), \\(p(\\rho, e)\\) is the gas pressure derived from a general convex equation of state, \\(E_g\\) is the radiation energy density for group \\(g\\) , \\(F_g\\) is the radiation flux for group \\(g\\) , \\(\\mathsf{P}_g\\) is the radiation pressure tensor for group \\(g\\) , \\(G_g\\) is the radiation four-force \\([G^0_g, \\vec{G}_g]\\) due to group \\(g\\) , \\(\\Delta S_{\\text{rad}}\\) is the change in gas internal energy due to radiation over a timestep, \\(\\phi\\) is the Newtonian gravitational potential, \\(\\vec{g}\\) is the gravitational acceleration, \\(\\rho_i\\) is the mass density due to particle \\(i\\) . Note that since work done by radiation on the gas is included in the \\(c \\sum_g G^0_g\\) term, \\(S_{\\text{rad}}\\) is not the same as \\(c \\sum_g G^0_g\\) .",
      "title": "Fluids and radiation"
    },
    {
      "location": "equations/#collisionless-particles",
      "text": "Quokka solves the following equation of motion for collisionless particles: \\[\\frac{d^2 \\vec{x}_i}{d t^2} = \\vec{g} ,\\] where \\(\\vec{x}_i\\) is the position vector of particle \\(i\\) .",
      "title": "Collisionless particles"
    },
    {
      "location": "error_checking/",
      "text": "Assertions and error checking AMReX assert macros AMReX provides several assertion macros: AMREX_ASSERT : Works when CMAKE_BUILD_TYPE=Debug . AMREX_ALWAYS_ASSERT : Always works on CPU. Works on GPU only if \"-DNDEBUG\" is NOT added to the compiler flags. Note that CMake adds \"-DNDEBUG\" by default when \"CMAKE_BUILD_TYPE=Release\". (See this GitHub discussion for details.) Abort Because the default CMake flags added in Release mode causes AMREX_ALWAYS_ASSERT not to function in GPU code, amrex::Abort is the best option to use if you want to abort a GPU kernel. amrex::Abort requires additional GPU register usage, so it should be used sparingly. The best strategy for error handling is often to set a value in an array that indicates an iterative solve failed in a given cell. (This is what Castro does for its nuclear burning networks.) For more details, see the AMReX documentation on assertions and error checking .",
      "title": "Assertions and error checking"
    },
    {
      "location": "error_checking/#assertions-and-error-checking",
      "text": "",
      "title": "Assertions and error checking"
    },
    {
      "location": "error_checking/#amrex-assert-macros",
      "text": "AMReX provides several assertion macros: AMREX_ASSERT : Works when CMAKE_BUILD_TYPE=Debug . AMREX_ALWAYS_ASSERT : Always works on CPU. Works on GPU only if \"-DNDEBUG\" is NOT added to the compiler flags. Note that CMake adds \"-DNDEBUG\" by default when \"CMAKE_BUILD_TYPE=Release\". (See this GitHub discussion for details.)",
      "title": "AMReX assert macros"
    },
    {
      "location": "error_checking/#abort",
      "text": "Because the default CMake flags added in Release mode causes AMREX_ALWAYS_ASSERT not to function in GPU code, amrex::Abort is the best option to use if you want to abort a GPU kernel. amrex::Abort requires additional GPU register usage, so it should be used sparingly. The best strategy for error handling is often to set a value in an array that indicates an iterative solve failed in a given cell. (This is what Castro does for its nuclear burning networks.) For more details, see the AMReX documentation on assertions and error checking .",
      "title": "Abort"
    },
    {
      "location": "flowchart/",
      "text": "Flowchart Download the flowchart as a PDF: quokka-flowchart.pdf",
      "title": "Flowchart"
    },
    {
      "location": "flowchart/#flowchart",
      "text": "Download the flowchart as a PDF: quokka-flowchart.pdf",
      "title": "Flowchart"
    },
    {
      "location": "howto_clang_tidy/",
      "text": "How to use clang-tidy clang-tidy is a command-line tool that automatically enforces certain aspects of code style and provides warnings for common programming mistakes. It automatically runs on every pull request in the Quokka GitHub repository. Using clang-tidy with VSCode The easiest way to use clang-tidy on your own computer is to install the clangd extension for Visual Studio Code (VSCode). (VSCode itself can be downloaded here .) Command-line alternative You can also run clang-tidy from the command line (see the documentation ). To see the clang-tidy warnings that are relevant only to the code changes you've made, you can use the clang-tidy-diff.py Python script .",
      "title": "How to use clang-tidy"
    },
    {
      "location": "howto_clang_tidy/#how-to-use-clang-tidy",
      "text": "clang-tidy is a command-line tool that automatically enforces certain aspects of code style and provides warnings for common programming mistakes. It automatically runs on every pull request in the Quokka GitHub repository.",
      "title": "How to use clang-tidy"
    },
    {
      "location": "howto_clang_tidy/#using-clang-tidy-with-vscode",
      "text": "The easiest way to use clang-tidy on your own computer is to install the clangd extension for Visual Studio Code (VSCode). (VSCode itself can be downloaded here .)",
      "title": "Using clang-tidy with VSCode"
    },
    {
      "location": "howto_clang_tidy/#command-line-alternative",
      "text": "You can also run clang-tidy from the command line (see the documentation ). To see the clang-tidy warnings that are relevant only to the code changes you've made, you can use the clang-tidy-diff.py Python script .",
      "title": "Command-line alternative"
    },
    {
      "location": "insitu_analysis/",
      "text": "In-situ analysis In-situ analysis refers to analyzing the simulations as they are running. There are two options: using the runtime diagnostics that are built-in to Quokka, and using Ascent , a third-party library. Diagnostics Most of Quokka's diagnostics are adapted from the implementation included in the Pele suite of AMReX-based combustion codes. (See the documentation for PeleLMeX diagnostics for an explanation of the original implementation.) There are three built-in diagnostics that can be configured to output at periodic intervals while the simulation is running: axis-aligned 2D projections axis-aligned 2D slices, and N-dimensional probability distribution functions (PDFs). 2D Projections This diagnostic outputs 2D axis-aligned projections as AMReX plotfiles prefixed with proj . Currently, using this diagnostic requires implementing a custom function in the problem generator for your simulation. (In the future, this diagnostic may be improved so that it can be configured entirely with runtime parameters.) The problem generator must call computePlaneProjection(F const &user_f, const int dir) where user_f is a lambda function that returns the value to project and dir is the axis along which the projection is taken. Example problem generator implementation: template <> auto RadhydroSimulation < ShockCloud >:: ComputeProjections ( const int dir ) const -> std :: unordered_map < std :: string , amrex :: BaseFab < amrex :: Real >> { // compute density projection std :: unordered_map < std :: string , amrex :: BaseFab < amrex :: Real >> proj ; proj [ \"nH\" ] = computePlaneProjection < amrex :: ReduceOpSum > ( [ = ] AMREX_GPU_DEVICE ( int i , int j , int k , amrex :: Array4 < const Real > const & state ) noexcept { Real const rho = state ( i , j , k , HydroSystem < ShockCloud >:: density_index ); return ( quokka :: cooling :: cloudy_H_mass_fraction * rho ) / m_H ; }, dir ); return proj ; } Example input file configuration: projection_interval = 200 projection.dirs = x z 2D Slices Note This is based on the DiagFramePlane diagnostic from PelePhysics, and the same runtime parameters should apply here without modification. The output format is also the same as that produced by the Pele codes. This outputs 2D slices of the simulation as AMReX plotfiles that can be further examined using, e.g., VisIt or yt. Example input file configuration: quokka.diagnostics = slice_z # Space-separated name(s) of diagnostics (arbitrary) quokka.slice_z.type = DiagFramePlane # Diagnostic type (others may be added in the future) quokka.slice_z.file = slicez_plt # Output file prefix (should end in \"plt\") quokka.slice_z.normal = 2 # Plane normal (0 == x, 1 == y, 2 == z) quokka.slice_z.center = 2.4688e20 # Coordinate in the normal direction quokka.slice_z.int = 10 # Output interval (in number of coarse steps) quokka.slice_z.interpolation = Linear # Interpolation type: Linear or Quadratic (default: Linear) # The problem must output these derived variable(s) derived_vars = temperature # List of variables to include in output quokka.slice_z.field_names = gasDensity gasInternalEnergy temperature Histograms/PDFs Note This is based on the DiagPDF diagnostic from PelePhysics, but significant changes have been made to both the runtime parameters and the output format in order to support N-dimensional histograms, log-spaced binning, and histogramming by mass. This adds histogram outputs (as fixed-width text files) at fixed timestep intervals as the simulation evolves. The quantity accumulated in each bin is the total mass, volume, or cell count summed over all cells not covered by refined grids over all AMR levels. If unspecified in the input parameters, the default is to accumulate the volume in each bin. By default, the bins extend over the full range of the data at a given timestep. The range parameter can instead specify the minimum and maximum extent for the bins. Bins can be optionally log-spaced by setting log_spaced_bins = 1 . Normalization of the output is left up to the user. Example input file configuration: quokka.hist_temp.type = DiagPDF # Diagnostic type quokka.hist_temp.file = PDFTempDens # Output file prefix quokka.hist_temp.int = 10 # Output cadence (in number of coarse steps) quokka.hist_temp.weight_by = mass # (Optional, default: volume) Accumulate: mass, volume, cell_counts quokka.hist_temp.var_names = temperature gasDensity # Variable(s) of interest (compute a N-D histogram) quokka.hist_temp.temperature.nBins = 20 # temperature: Number of bins quokka.hist_temp.temperature.log_spaced_bins = 1 # temperature: (Optional, default: 0) Use log-spaced bins quokka.hist_temp.temperature.range = 1e3 1e7 # temperature: (Optional, default: data range) Specify min/max of bins quokka.hist_temp.gasDensity.nBins = 5 # gasDensity: Number of bins quokka.hist_temp.gasDensity.log_spaced_bins = 1 # gasDensity: (Optional, default: 0) Use log-spaced bins quokka.hist_temp.gasDensity.range = 1e-29 1e-23 # gasDensity: (Optional, default: data range) Specify min/max of bins Filters (based on any variables, not necessary those used for the histogram) can be optionally added: quokka.hist_temp.filters = dense # (Optional) List of filters quokka.hist_temp.dense.field_name = gasDensity # Filter field quokka.hist_temp.dense.value_greater = 1e-25 # Filters: value_greater, value_less, value_inrange Ascent (deprecated) Warning Due to correctness and performance issues, using Ascent is not recommended . Support for Ascent will be removed in a future version of Quokka. Ascent allows you to generate visualizations (as PNG images) while the simulation is running, without any extra effort. Note On Setonix, Ascent is already built. In your job script, add the line: export Ascent_DIR=/software/projects/pawsey0807/bwibking/ascent_06082023/install/ascent-develop/lib/cmake/ascent . Compiling Ascent via Spack Run spack external find . Make sure there are entries listed for hdf5 , cuda , and openmpi in your ~/.spack/packages.yaml file. Add buildable: False to each entry. Run spack fetch --dependencies ascent@develop+cuda+vtkh~fortran~shared cuda_arch=70 ^conduit~parmetis~fortran On a dedicated compute node, run spack install ascent@develop+cuda+vtkh~fortran~shared cuda_arch=70 ^conduit~parmetis~fortran For A100 GPUs, change the above lines to cuda_arch=80 . Currently, it's not possible to build for both GPU models at the same time . Compiling Quokka with Ascent support Load Ascent: spack load ascent Add -DAMReX_ASCENT=ON -DAMReX_CONDUIT=ON to your CMake options. Compile your problem, e.g.: ninja -j4 test_hydro3d_blast Customizing the visualization Add an ascent_actions.yaml file to the simulation working directory. This file can even be edited while the simulation is running! Warning Volume renderings do not correctly handle ghost cells ( GitHub issue ).",
      "title": "In-situ analysis"
    },
    {
      "location": "insitu_analysis/#in-situ-analysis",
      "text": "In-situ analysis refers to analyzing the simulations as they are running. There are two options: using the runtime diagnostics that are built-in to Quokka, and using Ascent , a third-party library.",
      "title": "In-situ analysis"
    },
    {
      "location": "insitu_analysis/#diagnostics",
      "text": "Most of Quokka's diagnostics are adapted from the implementation included in the Pele suite of AMReX-based combustion codes. (See the documentation for PeleLMeX diagnostics for an explanation of the original implementation.) There are three built-in diagnostics that can be configured to output at periodic intervals while the simulation is running: axis-aligned 2D projections axis-aligned 2D slices, and N-dimensional probability distribution functions (PDFs).",
      "title": "Diagnostics"
    },
    {
      "location": "insitu_analysis/#2d-projections",
      "text": "This diagnostic outputs 2D axis-aligned projections as AMReX plotfiles prefixed with proj . Currently, using this diagnostic requires implementing a custom function in the problem generator for your simulation. (In the future, this diagnostic may be improved so that it can be configured entirely with runtime parameters.) The problem generator must call computePlaneProjection(F const &user_f, const int dir) where user_f is a lambda function that returns the value to project and dir is the axis along which the projection is taken. Example problem generator implementation: template <> auto RadhydroSimulation < ShockCloud >:: ComputeProjections ( const int dir ) const -> std :: unordered_map < std :: string , amrex :: BaseFab < amrex :: Real >> { // compute density projection std :: unordered_map < std :: string , amrex :: BaseFab < amrex :: Real >> proj ; proj [ \"nH\" ] = computePlaneProjection < amrex :: ReduceOpSum > ( [ = ] AMREX_GPU_DEVICE ( int i , int j , int k , amrex :: Array4 < const Real > const & state ) noexcept { Real const rho = state ( i , j , k , HydroSystem < ShockCloud >:: density_index ); return ( quokka :: cooling :: cloudy_H_mass_fraction * rho ) / m_H ; }, dir ); return proj ; } Example input file configuration: projection_interval = 200 projection.dirs = x z",
      "title": "2D Projections"
    },
    {
      "location": "insitu_analysis/#2d-slices",
      "text": "Note This is based on the DiagFramePlane diagnostic from PelePhysics, and the same runtime parameters should apply here without modification. The output format is also the same as that produced by the Pele codes. This outputs 2D slices of the simulation as AMReX plotfiles that can be further examined using, e.g., VisIt or yt. Example input file configuration: quokka.diagnostics = slice_z # Space-separated name(s) of diagnostics (arbitrary) quokka.slice_z.type = DiagFramePlane # Diagnostic type (others may be added in the future) quokka.slice_z.file = slicez_plt # Output file prefix (should end in \"plt\") quokka.slice_z.normal = 2 # Plane normal (0 == x, 1 == y, 2 == z) quokka.slice_z.center = 2.4688e20 # Coordinate in the normal direction quokka.slice_z.int = 10 # Output interval (in number of coarse steps) quokka.slice_z.interpolation = Linear # Interpolation type: Linear or Quadratic (default: Linear) # The problem must output these derived variable(s) derived_vars = temperature # List of variables to include in output quokka.slice_z.field_names = gasDensity gasInternalEnergy temperature",
      "title": "2D Slices"
    },
    {
      "location": "insitu_analysis/#histogramspdfs",
      "text": "Note This is based on the DiagPDF diagnostic from PelePhysics, but significant changes have been made to both the runtime parameters and the output format in order to support N-dimensional histograms, log-spaced binning, and histogramming by mass. This adds histogram outputs (as fixed-width text files) at fixed timestep intervals as the simulation evolves. The quantity accumulated in each bin is the total mass, volume, or cell count summed over all cells not covered by refined grids over all AMR levels. If unspecified in the input parameters, the default is to accumulate the volume in each bin. By default, the bins extend over the full range of the data at a given timestep. The range parameter can instead specify the minimum and maximum extent for the bins. Bins can be optionally log-spaced by setting log_spaced_bins = 1 . Normalization of the output is left up to the user. Example input file configuration: quokka.hist_temp.type = DiagPDF # Diagnostic type quokka.hist_temp.file = PDFTempDens # Output file prefix quokka.hist_temp.int = 10 # Output cadence (in number of coarse steps) quokka.hist_temp.weight_by = mass # (Optional, default: volume) Accumulate: mass, volume, cell_counts quokka.hist_temp.var_names = temperature gasDensity # Variable(s) of interest (compute a N-D histogram) quokka.hist_temp.temperature.nBins = 20 # temperature: Number of bins quokka.hist_temp.temperature.log_spaced_bins = 1 # temperature: (Optional, default: 0) Use log-spaced bins quokka.hist_temp.temperature.range = 1e3 1e7 # temperature: (Optional, default: data range) Specify min/max of bins quokka.hist_temp.gasDensity.nBins = 5 # gasDensity: Number of bins quokka.hist_temp.gasDensity.log_spaced_bins = 1 # gasDensity: (Optional, default: 0) Use log-spaced bins quokka.hist_temp.gasDensity.range = 1e-29 1e-23 # gasDensity: (Optional, default: data range) Specify min/max of bins Filters (based on any variables, not necessary those used for the histogram) can be optionally added: quokka.hist_temp.filters = dense # (Optional) List of filters quokka.hist_temp.dense.field_name = gasDensity # Filter field quokka.hist_temp.dense.value_greater = 1e-25 # Filters: value_greater, value_less, value_inrange",
      "title": "Histograms/PDFs"
    },
    {
      "location": "insitu_analysis/#ascent-deprecated",
      "text": "Warning Due to correctness and performance issues, using Ascent is not recommended . Support for Ascent will be removed in a future version of Quokka. Ascent allows you to generate visualizations (as PNG images) while the simulation is running, without any extra effort. Note On Setonix, Ascent is already built. In your job script, add the line: export Ascent_DIR=/software/projects/pawsey0807/bwibking/ascent_06082023/install/ascent-develop/lib/cmake/ascent .",
      "title": "Ascent (deprecated)"
    },
    {
      "location": "insitu_analysis/#compiling-ascent-via-spack",
      "text": "Run spack external find . Make sure there are entries listed for hdf5 , cuda , and openmpi in your ~/.spack/packages.yaml file. Add buildable: False to each entry. Run spack fetch --dependencies ascent@develop+cuda+vtkh~fortran~shared cuda_arch=70 ^conduit~parmetis~fortran On a dedicated compute node, run spack install ascent@develop+cuda+vtkh~fortran~shared cuda_arch=70 ^conduit~parmetis~fortran For A100 GPUs, change the above lines to cuda_arch=80 . Currently, it's not possible to build for both GPU models at the same time .",
      "title": "Compiling Ascent via Spack"
    },
    {
      "location": "insitu_analysis/#compiling-quokka-with-ascent-support",
      "text": "Load Ascent: spack load ascent Add -DAMReX_ASCENT=ON -DAMReX_CONDUIT=ON to your CMake options. Compile your problem, e.g.: ninja -j4 test_hydro3d_blast",
      "title": "Compiling Quokka with Ascent support"
    },
    {
      "location": "insitu_analysis/#customizing-the-visualization",
      "text": "Add an ascent_actions.yaml file to the simulation working directory. This file can even be edited while the simulation is running! Warning Volume renderings do not correctly handle ghost cells ( GitHub issue ).",
      "title": "Customizing the visualization"
    },
    {
      "location": "instability/",
      "text": "Debugging simulation instability Nonlinear stability of systems of PDEs is an unsolved problem. There is no complete, rigorous mathematical theory. There are two concepts, however, that are closely associated with nonlinear stability: positivity preservation: This is the property that, given a positive initial density and pressure at timestep \\(n\\) , the density and pressure are positive at timestep \\(n+1\\) . For theoretical background, see Linde & Roe (1997) . and Perthame & Shu (1996) . entropy stability: This is the property that the discretized system of equations obeys the second law of thermodynamics, i.e. the discrete entropy of the simulation must be non-decreasing. There is also a stronger local form, where the entropy variable everywhere obeys an entropy inequality. For theoretical background, see Harten (1983) and Tadmor (1986) . (This assumes a convex equation of state.) If a simulation goes unstable, it is likely due to one of the above properties being violated. It is important to note that standard finite volume reconstruction methods do not guarantee entropy stability (see Fjordholm et al. 2012 ). It is also possible that the entropy is nondecreasing, but insufficient entropy is produced for a given shock compared to the amount that should be produced physically. This will cause an unphysical oscillatory solution. Ways to improve stability The solution is either to reduce the timestep or add additional dissipation: set the initial timestep to be 0.1 or 0.01 of the CFL timestep by setting sim.initDt_ appropriately lower the CFL number It should be in the range 0.1-0.3. If it's above 0.3, it's linearly unstable, so it will never work. If it's below 0.1, it's sufficiently low that the simulation will be very inefficient. If it still doesn't work, experience indicates that reducing it further usually does not help. reduce the order of the spatial reconstruction By default PPM reconstruction is used, but PLM (with minmod limiter) can be used instead. It is much more dissipative, and therefore, stable. re-try the hydro update with a smaller timestep This is necessary because the positivity-preserving timestep may be much smaller than the CFL-limited timestep near the boundary of realizable states ( Linde & Roe 1997 ). Quokka will do this automatically, but only up to a maximum hard-coded number of retries. If the simulation still fails, this usually indicates a stability problem that will probably not be fixed by further timestep reductions. revert to a first-order update in problem cells For a sufficiently small timestep, this is provably entropy stable and positivity-preserving (as long as the Riemann solver itself is, which requires robust wavespeeds) Quokka reduces to first-order in space and time automatically when the density is negative in a given cell. In the future, Quokka could be extended to also revert to first-order based on entropy. use wavespeed estimates that are robust for strong shocks The eigenvalues of the Roe-average state do not provide correct bounds for very strong shocks. If the shocks at the interface travel faster than the wavespeed estimates, there will be insufficient entropy production. Doing this requires additional assumptions about the EOS ( Miller & Puckett 1996 ). Quokka attempts to do this for ideal gases and as well as materials that can be approximated with a Mie-Gruniesen EOS (see Dukowicz 1985 and Rider 1999 ). No code changes should be required unless you are simulating an exotic material or a condensed matter phase transition (gaseous phase transitions do not cause any issues; see Bethe 1942 ). add artificial viscosity This can be helpful because it adds dissipation when shocks are propagating transverse to the interface. For sufficient entropy production, it is important that the velocity divergence estimator is based on the cell-average velocities surrounding the interface, not the reconstructed velocities. This can be enabled in Quokka with the runtime parameter hydro.artificial_viscosity_coefficient . A value of 0.1 is recommended. This parameter is identical to the artificial viscosity coefficient described in Colella and Woodward 1984 . Floors As an absolute last resort, one can enable density and/or temperature floors for a simulation using Quokka's EnforceLimits function. This may be necessary if the positivity-preserving timestep for a state near vacuum is too small to be feasible. A temperature floor may also be necessary in order to prevent the auxiliary internal energy from becoming negative when there is strong cooling.",
      "title": "Debugging simulation instability"
    },
    {
      "location": "instability/#debugging-simulation-instability",
      "text": "Nonlinear stability of systems of PDEs is an unsolved problem. There is no complete, rigorous mathematical theory. There are two concepts, however, that are closely associated with nonlinear stability: positivity preservation: This is the property that, given a positive initial density and pressure at timestep \\(n\\) , the density and pressure are positive at timestep \\(n+1\\) . For theoretical background, see Linde & Roe (1997) . and Perthame & Shu (1996) . entropy stability: This is the property that the discretized system of equations obeys the second law of thermodynamics, i.e. the discrete entropy of the simulation must be non-decreasing. There is also a stronger local form, where the entropy variable everywhere obeys an entropy inequality. For theoretical background, see Harten (1983) and Tadmor (1986) . (This assumes a convex equation of state.) If a simulation goes unstable, it is likely due to one of the above properties being violated. It is important to note that standard finite volume reconstruction methods do not guarantee entropy stability (see Fjordholm et al. 2012 ). It is also possible that the entropy is nondecreasing, but insufficient entropy is produced for a given shock compared to the amount that should be produced physically. This will cause an unphysical oscillatory solution.",
      "title": "Debugging simulation instability"
    },
    {
      "location": "instability/#ways-to-improve-stability",
      "text": "The solution is either to reduce the timestep or add additional dissipation: set the initial timestep to be 0.1 or 0.01 of the CFL timestep by setting sim.initDt_ appropriately lower the CFL number It should be in the range 0.1-0.3. If it's above 0.3, it's linearly unstable, so it will never work. If it's below 0.1, it's sufficiently low that the simulation will be very inefficient. If it still doesn't work, experience indicates that reducing it further usually does not help. reduce the order of the spatial reconstruction By default PPM reconstruction is used, but PLM (with minmod limiter) can be used instead. It is much more dissipative, and therefore, stable. re-try the hydro update with a smaller timestep This is necessary because the positivity-preserving timestep may be much smaller than the CFL-limited timestep near the boundary of realizable states ( Linde & Roe 1997 ). Quokka will do this automatically, but only up to a maximum hard-coded number of retries. If the simulation still fails, this usually indicates a stability problem that will probably not be fixed by further timestep reductions. revert to a first-order update in problem cells For a sufficiently small timestep, this is provably entropy stable and positivity-preserving (as long as the Riemann solver itself is, which requires robust wavespeeds) Quokka reduces to first-order in space and time automatically when the density is negative in a given cell. In the future, Quokka could be extended to also revert to first-order based on entropy. use wavespeed estimates that are robust for strong shocks The eigenvalues of the Roe-average state do not provide correct bounds for very strong shocks. If the shocks at the interface travel faster than the wavespeed estimates, there will be insufficient entropy production. Doing this requires additional assumptions about the EOS ( Miller & Puckett 1996 ). Quokka attempts to do this for ideal gases and as well as materials that can be approximated with a Mie-Gruniesen EOS (see Dukowicz 1985 and Rider 1999 ). No code changes should be required unless you are simulating an exotic material or a condensed matter phase transition (gaseous phase transitions do not cause any issues; see Bethe 1942 ). add artificial viscosity This can be helpful because it adds dissipation when shocks are propagating transverse to the interface. For sufficient entropy production, it is important that the velocity divergence estimator is based on the cell-average velocities surrounding the interface, not the reconstructed velocities. This can be enabled in Quokka with the runtime parameter hydro.artificial_viscosity_coefficient . A value of 0.1 is recommended. This parameter is identical to the artificial viscosity coefficient described in Colella and Woodward 1984 .",
      "title": "Ways to improve stability"
    },
    {
      "location": "instability/#floors",
      "text": "As an absolute last resort, one can enable density and/or temperature floors for a simulation using Quokka's EnforceLimits function. This may be necessary if the positivity-preserving timestep for a state near vacuum is too small to be feasible. A temperature floor may also be necessary in order to prevent the auxiliary internal energy from becoming negative when there is strong cooling.",
      "title": "Floors"
    },
    {
      "location": "installation/",
      "text": "Installation To run Quokka, download this repository and its submodules to your local machine: git clone --recursive https://github.com/quokka-astro/quokka.git Quokka uses CMake (and optionally, Ninja) as its build system. If you don't have CMake and Ninja installed, the easiest way to install them is to run: python3 -m pip install cmake ninja --user Now that CMake is installed, create a build/ subdirectory and compile Quokka, as shown below. cd quokka mkdir build; cd build cmake .. -DCMAKE_BUILD_TYPE=Release -G Ninja ninja -j6 Congratuations! You have now built all of the 1D test problems on CPU. You can run the automated test suite: ninja test You should see output that indicates all tests have passed, like this: 100% tests passed, 0 tests failed out of 20 Total Test time (real) = 111.74 sec To run in 2D or 3D, build with the -DAMReX_SPACEDIM CMake option, for example: cmake .. -DCMAKE_BUILD_TYPE=Release -DAMReX_SPACEDIM=3 -G Ninja ninja -j6 to compile Quokka for 3D problems. By default, Quokka compiles itself only for CPUs. If you want to run Quokka on GPUs, see the section \"Running on GPUs\" below. Have fun! Building with CMake + make If you are unable to install Ninja, you can instead use CMake with the Makefile generator, which should produce identical results but is slower: cmake .. -DCMAKE_BUILD_TYPE=Release -G \"Unix Makefiles\" make -j6 make test Could NOT find Python error If CMake prints an error saying that Python could not be found, e.g.: -- Could NOT find Python (missing: Python_EXECUTABLE Python_INCLUDE_DIRS Python_LIBRARIES Python_NumPy_INCLUDE_DIRS Interpreter Development NumPy Development.Module Development.Embed) you should be able to fix this by installing NumPy (and matplotlib) by running python3 -m pip install numpy matplotlib --user This should enable CMake to find the NumPy header files that are needed to successfully compile. Alternatively, you can work around this problem by disabling Python support. Python and NumPy are only used to plot the results of some test problems, so this does not otherwise affect Quokka's functionality. Add the option -DQUOKKA_PYTHON=OFF to the CMake command-line options (or change the QUOKKA_PYTHON option to OFF in CMakeLists.txt). Running on GPUs By default, Quokka compiles itself to run only on CPUs. Quokka can run on either NVIDIA or AMD GPUs. Consult the sub-sections below for the build instructions for a given GPU vendor. NVIDIA GPUs If you want to run on NVIDIA GPUs, re-build Quokka as shown below. ( CUDA >= 11.7 is required. Quokka is only supported on Volta V100 GPUs or newer models. Your MPI library must support CUDA-aware MPI. ) cmake .. -DCMAKE_BUILD_TYPE=Release -DAMReX_GPU_BACKEND=CUDA -DAMReX_SPACEDIM=3 -G Ninja ninja -j6 All GPUs on a node must be visible from each MPI rank on the node for efficient GPU-aware MPI communication to take place via CUDA IPC. When using the SLURM job scheduler, this means that --gpu-bind should be set to none . The compiled test problems are in the test problem subdirectories in build/src/ . Example scripts for running Quokka on compute clusters are in the scripts/ subdirectory. Note that 1D problems can run very slowly on GPUs due to a lack of sufficient parallelism. To run the test suite in a reasonable amount of time, you may wish to exclude the matter-energy exchange tests, e.g.: ctest -E \"MatterEnergyExchange*\" which should end with output similar to the following: 100% tests passed, 0 tests failed out of 18 Total Test time (real) = 353.77 sec AMD GPUs (experimental, use at your own risk) Compile with -DAMReX_GPU_BACKEND=HIP . Requires ROCm 5.2.0 or newer. Your MPI library must support GPU-aware MPI for AMD GPUs. Quokka has been tested on MI100 and MI250X GPUs, but there are known compiler issues that affect the correctness of simulation results (see https://github.com/quokka-astro/quokka/issues/394 and https://github.com/quokka-astro/quokka/issues/447 ). Intel GPUs (does not compile) Due to limitations in the Intel GPU programming model, Quokka currently cannot be compiled for Intel GPUs. (See https://github.com/quokka-astro/quokka/issues/619 for the technical details.) Building a specific test problem By default, all available test problems will be compiled. If you only want to build a specific problem, you can list all of the available CMake targets: cmake --build . --target help and then build the problem of interest: ninja -j6 test_hydro3d_blast",
      "title": "Installation"
    },
    {
      "location": "installation/#installation",
      "text": "To run Quokka, download this repository and its submodules to your local machine: git clone --recursive https://github.com/quokka-astro/quokka.git Quokka uses CMake (and optionally, Ninja) as its build system. If you don't have CMake and Ninja installed, the easiest way to install them is to run: python3 -m pip install cmake ninja --user Now that CMake is installed, create a build/ subdirectory and compile Quokka, as shown below. cd quokka mkdir build; cd build cmake .. -DCMAKE_BUILD_TYPE=Release -G Ninja ninja -j6 Congratuations! You have now built all of the 1D test problems on CPU. You can run the automated test suite: ninja test You should see output that indicates all tests have passed, like this: 100% tests passed, 0 tests failed out of 20 Total Test time (real) = 111.74 sec To run in 2D or 3D, build with the -DAMReX_SPACEDIM CMake option, for example: cmake .. -DCMAKE_BUILD_TYPE=Release -DAMReX_SPACEDIM=3 -G Ninja ninja -j6 to compile Quokka for 3D problems. By default, Quokka compiles itself only for CPUs. If you want to run Quokka on GPUs, see the section \"Running on GPUs\" below. Have fun!",
      "title": "Installation"
    },
    {
      "location": "installation/#building-with-cmake-make",
      "text": "If you are unable to install Ninja, you can instead use CMake with the Makefile generator, which should produce identical results but is slower: cmake .. -DCMAKE_BUILD_TYPE=Release -G \"Unix Makefiles\" make -j6 make test",
      "title": "Building with CMake + make"
    },
    {
      "location": "installation/#could-not-find-python-error",
      "text": "If CMake prints an error saying that Python could not be found, e.g.: -- Could NOT find Python (missing: Python_EXECUTABLE Python_INCLUDE_DIRS Python_LIBRARIES Python_NumPy_INCLUDE_DIRS Interpreter Development NumPy Development.Module Development.Embed) you should be able to fix this by installing NumPy (and matplotlib) by running python3 -m pip install numpy matplotlib --user This should enable CMake to find the NumPy header files that are needed to successfully compile. Alternatively, you can work around this problem by disabling Python support. Python and NumPy are only used to plot the results of some test problems, so this does not otherwise affect Quokka's functionality. Add the option -DQUOKKA_PYTHON=OFF to the CMake command-line options (or change the QUOKKA_PYTHON option to OFF in CMakeLists.txt).",
      "title": "Could NOT find Python error"
    },
    {
      "location": "installation/#running-on-gpus",
      "text": "By default, Quokka compiles itself to run only on CPUs. Quokka can run on either NVIDIA or AMD GPUs. Consult the sub-sections below for the build instructions for a given GPU vendor.",
      "title": "Running on GPUs"
    },
    {
      "location": "installation/#nvidia-gpus",
      "text": "If you want to run on NVIDIA GPUs, re-build Quokka as shown below. ( CUDA >= 11.7 is required. Quokka is only supported on Volta V100 GPUs or newer models. Your MPI library must support CUDA-aware MPI. ) cmake .. -DCMAKE_BUILD_TYPE=Release -DAMReX_GPU_BACKEND=CUDA -DAMReX_SPACEDIM=3 -G Ninja ninja -j6 All GPUs on a node must be visible from each MPI rank on the node for efficient GPU-aware MPI communication to take place via CUDA IPC. When using the SLURM job scheduler, this means that --gpu-bind should be set to none . The compiled test problems are in the test problem subdirectories in build/src/ . Example scripts for running Quokka on compute clusters are in the scripts/ subdirectory. Note that 1D problems can run very slowly on GPUs due to a lack of sufficient parallelism. To run the test suite in a reasonable amount of time, you may wish to exclude the matter-energy exchange tests, e.g.: ctest -E \"MatterEnergyExchange*\" which should end with output similar to the following: 100% tests passed, 0 tests failed out of 18 Total Test time (real) = 353.77 sec",
      "title": "NVIDIA GPUs"
    },
    {
      "location": "installation/#amd-gpus-experimental-use-at-your-own-risk",
      "text": "Compile with -DAMReX_GPU_BACKEND=HIP . Requires ROCm 5.2.0 or newer. Your MPI library must support GPU-aware MPI for AMD GPUs. Quokka has been tested on MI100 and MI250X GPUs, but there are known compiler issues that affect the correctness of simulation results (see https://github.com/quokka-astro/quokka/issues/394 and https://github.com/quokka-astro/quokka/issues/447 ).",
      "title": "AMD GPUs (experimental, use at your own risk)"
    },
    {
      "location": "installation/#intel-gpus-does-not-compile",
      "text": "Due to limitations in the Intel GPU programming model, Quokka currently cannot be compiled for Intel GPUs. (See https://github.com/quokka-astro/quokka/issues/619 for the technical details.)",
      "title": "Intel GPUs (does not compile)"
    },
    {
      "location": "installation/#building-a-specific-test-problem",
      "text": "By default, all available test problems will be compiled. If you only want to build a specific problem, you can list all of the available CMake targets: cmake --build . --target help and then build the problem of interest: ninja -j6 test_hydro3d_blast",
      "title": "Building a specific test problem"
    },
    {
      "location": "parameters/",
      "text": "Runtime parameters This document lists all of the runtime parameters in Quokka that are set using the AMReX ParmParse object. Users can set these via the input file or by using command-line arguments. General These parameters are read in the AMRSimulation<problem_t>::readParameters() function in src/simulation.hpp . Parameter Name Type Description max_timesteps Integer The maximum number of time steps for the simulation. cfl Float Sets the CFL number for the simulation. amr_interpolation_method Integer Selects the method (piecewise constant or piecewise linear with limiters) used to interpolate from coarse to fine AMR levels. Except for debugging, this should not be changed. stop_time Float The simulation time at which to stop evolving the simulation. ascent_interval Integer The number of coarse timesteps between Ascent outputs. plotfile_interval Integer The number of coarse timesteps between plotfile outputs. plottime_interval Float The time interval (in simulated time) between plotfile outputs. projection_interval Integer The number of coarse timesteps between 2D projection outputs. statistics_interval Integer The number of coarse timesteps between statistics outputs. checkpoint_interval Float The number of coarse timesteps between checkpoint outputs. checkpointtime_interval Float The time interval (in simulated time) between checkpoint outputs. do_reflux Integer This turns on refluxing at coarse-fine boundaries (1) or turns it off (0). Except for debugging, this should always be on when AMR is used. do_tracers Integer This turns on tracer particles. They are initialized one-per-cell and they follow the fluid velocity. Default: 0 (off). suppress_output Integer If set to 1, this disables output to stdout while the simulation is running. derived_vars String A list of the names of derived variables that should be included in the plotfile and Ascent outputs. regrid_interval Integer The number of timesteps between AMR regridding. density_floor Float The minimum density value allowed in the simulation. Enforced through EnforceLimits. temperature_floor Float The minimum temperature value allowed in the simulation. Enforced through EnforceLimits. max_walltime String The maximum walltime for the simulation in the format DD:HH:SS (days/hours/seconds). After 90% of this walltime elapses, the simulation will automatically stop and exit. Hydrodynamics These parameters are read in the RadhydroSimulation<problem_t>::readParmParse() function in src/RadhydroSimulation.hpp . Parameter Name Type Description hydro.low_level_debugging_output Integer If set to 1, turns on low-level debugging output for each RK stage. Warning: this writes an enormous volume of data to disk! This should only be used for debugging. Default: 0. hydro.rk_integrator_order Integer Determines the order of the RK integrator used. Can be set to 1 (Forward Euler) or 2 (RK2-SSP, also known as Heun's method). Default: 2. This should only be changed for debugging. hydro.reconstruction_order Integer Determines the order of spatial reconstruction algorithm used. Can be set to 1 (piecewise constant), 2 (piecewise linear; PLM), or 3 (piecewise parabolic; PPM). Default: 3 (PPM). hydro.use_dual_energy Integer If set to 1, the code evolves an auxiliary internal energy variable in order to correctly evolve high-mach flows. This should only be disabled (0) for debugging. Default: 1. hydro.abort_on_fofc_failure Integer If set to 1, the code aborts when first-order flux correction fails to yield a physical state (positive density and pressure). This should only be disabled (0) for debugging. hydro.artificial_viscosity_coefficient Float This is the linear artificial viscosity coefficient used in the artificial viscosity term added to the flux. This is the same parameter as defined in the original PPM paper. Default: 0. Radiation These parameters are read in the RadhydroSimulation<problem_t>::readParmParse() function in src/RadhydroSimulation.hpp . Parameter Name Type Description radiation.reconstruction_order Integer Determines the order of spatial reconstruction algorithm used. Can be set to 1 (piecewise constant), 2 (piecewise linear; PLM), or 3 (piecewise parabolic; PPM). Default: 3 (PPM). radiation.cfl Float Sets the CFL number for the radiation advance. This is independent of the hydro CFL number. Optically-thin radiative cooling These parameters are read in the RadhydroSimulation<problem_t>::readParmParse() function in src/RadhydroSimulation.hpp . Parameter Name Type Description cooling.enabled Integer If set to 1, turns on optically-thin radiative cooling as a Strang-split source term. Default: 0 (disabled). cooling.read_tables_even_if_disabled Integer If set to 1, reads the cooling tables even if the cooling module is disabled. cooling.grackle_data_file String The path to the cooling tables in Grackle-compatible HDF5 format.",
      "title": "Runtime parameters"
    },
    {
      "location": "parameters/#runtime-parameters",
      "text": "This document lists all of the runtime parameters in Quokka that are set using the AMReX ParmParse object. Users can set these via the input file or by using command-line arguments.",
      "title": "Runtime parameters"
    },
    {
      "location": "parameters/#general",
      "text": "These parameters are read in the AMRSimulation<problem_t>::readParameters() function in src/simulation.hpp . Parameter Name Type Description max_timesteps Integer The maximum number of time steps for the simulation. cfl Float Sets the CFL number for the simulation. amr_interpolation_method Integer Selects the method (piecewise constant or piecewise linear with limiters) used to interpolate from coarse to fine AMR levels. Except for debugging, this should not be changed. stop_time Float The simulation time at which to stop evolving the simulation. ascent_interval Integer The number of coarse timesteps between Ascent outputs. plotfile_interval Integer The number of coarse timesteps between plotfile outputs. plottime_interval Float The time interval (in simulated time) between plotfile outputs. projection_interval Integer The number of coarse timesteps between 2D projection outputs. statistics_interval Integer The number of coarse timesteps between statistics outputs. checkpoint_interval Float The number of coarse timesteps between checkpoint outputs. checkpointtime_interval Float The time interval (in simulated time) between checkpoint outputs. do_reflux Integer This turns on refluxing at coarse-fine boundaries (1) or turns it off (0). Except for debugging, this should always be on when AMR is used. do_tracers Integer This turns on tracer particles. They are initialized one-per-cell and they follow the fluid velocity. Default: 0 (off). suppress_output Integer If set to 1, this disables output to stdout while the simulation is running. derived_vars String A list of the names of derived variables that should be included in the plotfile and Ascent outputs. regrid_interval Integer The number of timesteps between AMR regridding. density_floor Float The minimum density value allowed in the simulation. Enforced through EnforceLimits. temperature_floor Float The minimum temperature value allowed in the simulation. Enforced through EnforceLimits. max_walltime String The maximum walltime for the simulation in the format DD:HH:SS (days/hours/seconds). After 90% of this walltime elapses, the simulation will automatically stop and exit.",
      "title": "General"
    },
    {
      "location": "parameters/#hydrodynamics",
      "text": "These parameters are read in the RadhydroSimulation<problem_t>::readParmParse() function in src/RadhydroSimulation.hpp . Parameter Name Type Description hydro.low_level_debugging_output Integer If set to 1, turns on low-level debugging output for each RK stage. Warning: this writes an enormous volume of data to disk! This should only be used for debugging. Default: 0. hydro.rk_integrator_order Integer Determines the order of the RK integrator used. Can be set to 1 (Forward Euler) or 2 (RK2-SSP, also known as Heun's method). Default: 2. This should only be changed for debugging. hydro.reconstruction_order Integer Determines the order of spatial reconstruction algorithm used. Can be set to 1 (piecewise constant), 2 (piecewise linear; PLM), or 3 (piecewise parabolic; PPM). Default: 3 (PPM). hydro.use_dual_energy Integer If set to 1, the code evolves an auxiliary internal energy variable in order to correctly evolve high-mach flows. This should only be disabled (0) for debugging. Default: 1. hydro.abort_on_fofc_failure Integer If set to 1, the code aborts when first-order flux correction fails to yield a physical state (positive density and pressure). This should only be disabled (0) for debugging. hydro.artificial_viscosity_coefficient Float This is the linear artificial viscosity coefficient used in the artificial viscosity term added to the flux. This is the same parameter as defined in the original PPM paper. Default: 0.",
      "title": "Hydrodynamics"
    },
    {
      "location": "parameters/#radiation",
      "text": "These parameters are read in the RadhydroSimulation<problem_t>::readParmParse() function in src/RadhydroSimulation.hpp . Parameter Name Type Description radiation.reconstruction_order Integer Determines the order of spatial reconstruction algorithm used. Can be set to 1 (piecewise constant), 2 (piecewise linear; PLM), or 3 (piecewise parabolic; PPM). Default: 3 (PPM). radiation.cfl Float Sets the CFL number for the radiation advance. This is independent of the hydro CFL number.",
      "title": "Radiation"
    },
    {
      "location": "parameters/#optically-thin-radiative-cooling",
      "text": "These parameters are read in the RadhydroSimulation<problem_t>::readParmParse() function in src/RadhydroSimulation.hpp . Parameter Name Type Description cooling.enabled Integer If set to 1, turns on optically-thin radiative cooling as a Strang-split source term. Default: 0 (disabled). cooling.read_tables_even_if_disabled Integer If set to 1, reads the cooling tables even if the cooling module is disabled. cooling.grackle_data_file String The path to the cooling tables in Grackle-compatible HDF5 format.",
      "title": "Optically-thin radiative cooling"
    },
    {
      "location": "performance/",
      "text": "Performance tips Prerequisites You should: Understand what a GPU kernel is. (For reference, consult these notes .) Understand what a processor register is. Know that calling amrex::ParallelFor launches a GPU kernel (when GPU support is enabled at compile time). GPU hardware characteristics GPUs have hardware design features that make their performance characteristics significantly different from CPUs. In practice, two factors dominate GPU performance behavior: Kernel launch latency: this is a fundamental hardware characteristic of GPUs. It takes several microseconds (typically 3-10 microseconds, but it can vary depending on the compute kernel, the GPU hardware, the CPU hardware, and the driver) to launch a GPU kernel (i.e., to start running the code within an amrex::ParallelFor on the GPU). In practice, latency is generally longer for AMD and Intel GPUs. Register pressure: the number of registers per thread available for use by a given kernel is limited to the size of the GPU register file divided by the number of threads. If a kernel needs more registers than are available in the register file, the compiler will \"spill\" registers to memory, which will then make the kernel run very slowly. Alternatively, the number of concurrent threads can be reduced, which increases the number of registers available per thread. For more details, see these AMD website notes and OLCF training materials . MPI communication latency vs. bandwidth A traditional rule of thumb for CPU-based MPI codes is that communication latency often limits performance when scaling to large number of CPU cores (or, equivalently, MPI ranks). We have found that this is not the case for Quokka when running on GPU nodes (by, e.g., adding additional dummy variables to the state arrays). Communication performance appears to (virtually always) be bandwidth limited . There are two likely reasons for this: GPU node performance is about 10x faster than CPU node performance, whereas network bandwidth is only 2-4x larger on GPU nodes compared to CPU nodes. The network bandwidth to compute ratio is therefore lower on GPU nodes than on CPU nodes. GPU kernel launch latency (3-10 microseconds) is often larger than the minimum MPI message latency (i.e., the latency for small messages to travel between nodes) of 2-3 microseconds. Guidelines Combine operations into fewer kernels in order to reduce the fraction of time lost to kernel launch latency This can also be done by using the MultiFab version of ParallelFor that operates on all of the FABs at once, rather than launching a separate kernel for each FAB. This should not increase register pressure. However, combining multiple kernels can increase register pressure, which can decrease performance. There is no real way to know a priori whether there will be a net performance gain or loss without trying it out. The strategy that yields the best performance may be different for GPUs from different vendors! Split operations into multiple kernels in order to decrease register pressure However, this may increase the time lost due to kernel launch latency. This is an engineering trade-off that must be determined by performance measurements on the GPU hardware. This trade-off may be different on GPUs from different vendors! In order to decrease register pressure, avoid using printf , assert , and amrex::Abort in GPU code . All of these functions require using additional registers that could instead be allocated to the useful computations does in a kernel. This may require a significant code rewrite to handle errors in a different way. (You should not just ignore errors, e.g. in an iterative solver.) Experts only: Manually tune the number of GPU threads per block on a kernel-by-kernel basis. This can reduce register pressure by allowing each thread to use more registers. Note that this is an advanced optimization and should only be done with careful performance measurements done on multiple GPUs. The AMReX documentation provides guidance on how to do this.",
      "title": "Performance tips"
    },
    {
      "location": "performance/#performance-tips",
      "text": "",
      "title": "Performance tips"
    },
    {
      "location": "performance/#prerequisites",
      "text": "You should: Understand what a GPU kernel is. (For reference, consult these notes .) Understand what a processor register is. Know that calling amrex::ParallelFor launches a GPU kernel (when GPU support is enabled at compile time).",
      "title": "Prerequisites"
    },
    {
      "location": "performance/#gpu-hardware-characteristics",
      "text": "GPUs have hardware design features that make their performance characteristics significantly different from CPUs. In practice, two factors dominate GPU performance behavior: Kernel launch latency: this is a fundamental hardware characteristic of GPUs. It takes several microseconds (typically 3-10 microseconds, but it can vary depending on the compute kernel, the GPU hardware, the CPU hardware, and the driver) to launch a GPU kernel (i.e., to start running the code within an amrex::ParallelFor on the GPU). In practice, latency is generally longer for AMD and Intel GPUs. Register pressure: the number of registers per thread available for use by a given kernel is limited to the size of the GPU register file divided by the number of threads. If a kernel needs more registers than are available in the register file, the compiler will \"spill\" registers to memory, which will then make the kernel run very slowly. Alternatively, the number of concurrent threads can be reduced, which increases the number of registers available per thread. For more details, see these AMD website notes and OLCF training materials .",
      "title": "GPU hardware characteristics"
    },
    {
      "location": "performance/#mpi-communication-latency-vs-bandwidth",
      "text": "A traditional rule of thumb for CPU-based MPI codes is that communication latency often limits performance when scaling to large number of CPU cores (or, equivalently, MPI ranks). We have found that this is not the case for Quokka when running on GPU nodes (by, e.g., adding additional dummy variables to the state arrays). Communication performance appears to (virtually always) be bandwidth limited . There are two likely reasons for this: GPU node performance is about 10x faster than CPU node performance, whereas network bandwidth is only 2-4x larger on GPU nodes compared to CPU nodes. The network bandwidth to compute ratio is therefore lower on GPU nodes than on CPU nodes. GPU kernel launch latency (3-10 microseconds) is often larger than the minimum MPI message latency (i.e., the latency for small messages to travel between nodes) of 2-3 microseconds.",
      "title": "MPI communication latency vs. bandwidth"
    },
    {
      "location": "performance/#guidelines",
      "text": "Combine operations into fewer kernels in order to reduce the fraction of time lost to kernel launch latency This can also be done by using the MultiFab version of ParallelFor that operates on all of the FABs at once, rather than launching a separate kernel for each FAB. This should not increase register pressure. However, combining multiple kernels can increase register pressure, which can decrease performance. There is no real way to know a priori whether there will be a net performance gain or loss without trying it out. The strategy that yields the best performance may be different for GPUs from different vendors! Split operations into multiple kernels in order to decrease register pressure However, this may increase the time lost due to kernel launch latency. This is an engineering trade-off that must be determined by performance measurements on the GPU hardware. This trade-off may be different on GPUs from different vendors! In order to decrease register pressure, avoid using printf , assert , and amrex::Abort in GPU code . All of these functions require using additional registers that could instead be allocated to the useful computations does in a kernel. This may require a significant code rewrite to handle errors in a different way. (You should not just ignore errors, e.g. in an iterative solver.) Experts only: Manually tune the number of GPU threads per block on a kernel-by-kernel basis. This can reduce register pressure by allowing each thread to use more registers. Note that this is an advanced optimization and should only be done with careful performance measurements done on multiple GPUs. The AMReX documentation provides guidance on how to do this.",
      "title": "Guidelines"
    },
    {
      "location": "postprocessing/",
      "text": "Postprocessing There are several ways to post-process the output of Quokka simulations. AMReX PlotfileTools, yt, and VisIt all allow you to analyze the outputs after they are written to disk. AMReX PlotfileTools These are self-contained C++ programs (included with AMReX in the Tools/Plotfile subdirectory) that will output a 2D slice (axis-aligned), a 1D slice (axis-aligned), or compute a volume integral given an AMReX plotfile. For these tasks, it is almost always easier to use the Plotfile tools rather than, e.g., yt or VisIt. To compute a volume integral, use fvolumesum . To compute a 2D slice plot (axis-aligned planes only), use fsnapshot . To compute a 1D slice (axis-aligned directions only, with output as ASCII), use fextract . Other tools: fboxinfo prints out the indices of all the Boxes in a plotfile fcompare calculates the absolute and relative errors between plotfiles in L-inf norm fextrema calculates the minimum and maximum values of all variables in a plotfile fnan determines whether there are any NaNs in a plotfile ftime prints the simulation time of each plotfile fvarnames prints the names of all the variables in a given plotfile yt Warning There are known bugs that affect Quokka outputs. PlotfileTools (see above) can be used instead for axis-aligned slice plots. The plotfile directory can be loaded with yt.load as usual. However, the standard fields such as ('gas', 'density') are not defined. Instead, you have to use non-standard fields. Examine ds.field_list to see the fields that exist in the plotfiles. These should be: [( 'boxlib' , 'gasDensity' ), ( 'boxlib' , 'gasEnergy' ), ( 'boxlib' , 'radEnergy' ), ( 'boxlib' , 'scalar' ), ( 'boxlib' , 'temperature' ), ( 'boxlib' , 'x-GasMomentum' ), ( 'boxlib' , 'x-RadFlux' ), ( 'boxlib' , 'y-GasMomentum' ), ( 'boxlib' , 'y-RadFlux' ), ( 'boxlib' , 'z-GasMomentum' ), ( 'boxlib' , 'z-RadFlux' )] For details, see the yt documentation on reading AMReX data . Tip One of the most useful things to do is to convert the data into a uniform-resolution NumPy array with the covering_grid function. Tip This WarpX script may be useful as a starting point for visualizing a time series of outputs. This script will require some modification to work with Quokka outputs. VisIt VisIt can read cell-centered output variables from AMReX plotfiles. Currently, there is no support for reading either face-centered variables or particles. (However, by default, cell-centered averages of face-centered variables are included in Quokka plotfiles.) In order to read an individual plotfile, you can select the plt00000/Header file in VisIt's Open dialog box. If you want to read a timeseries of plotfiles, you can create a file with a .visit extension that lists the plt*/Header files, one per line, with the following command: : ls -1 plt*/Header | tee plotfiles.visit Then select plotfiles.visit in VisIt's Open dialog box. Warning There are rendering bugs with unscaled box dimensions. Slices generally work. However, do not expect volume rendering to work when using, e.g. parsec-size boxes with cgs units.",
      "title": "Postprocessing"
    },
    {
      "location": "postprocessing/#postprocessing",
      "text": "There are several ways to post-process the output of Quokka simulations. AMReX PlotfileTools, yt, and VisIt all allow you to analyze the outputs after they are written to disk.",
      "title": "Postprocessing"
    },
    {
      "location": "postprocessing/#amrex-plotfiletools",
      "text": "These are self-contained C++ programs (included with AMReX in the Tools/Plotfile subdirectory) that will output a 2D slice (axis-aligned), a 1D slice (axis-aligned), or compute a volume integral given an AMReX plotfile. For these tasks, it is almost always easier to use the Plotfile tools rather than, e.g., yt or VisIt. To compute a volume integral, use fvolumesum . To compute a 2D slice plot (axis-aligned planes only), use fsnapshot . To compute a 1D slice (axis-aligned directions only, with output as ASCII), use fextract . Other tools: fboxinfo prints out the indices of all the Boxes in a plotfile fcompare calculates the absolute and relative errors between plotfiles in L-inf norm fextrema calculates the minimum and maximum values of all variables in a plotfile fnan determines whether there are any NaNs in a plotfile ftime prints the simulation time of each plotfile fvarnames prints the names of all the variables in a given plotfile",
      "title": "AMReX PlotfileTools"
    },
    {
      "location": "postprocessing/#yt",
      "text": "Warning There are known bugs that affect Quokka outputs. PlotfileTools (see above) can be used instead for axis-aligned slice plots. The plotfile directory can be loaded with yt.load as usual. However, the standard fields such as ('gas', 'density') are not defined. Instead, you have to use non-standard fields. Examine ds.field_list to see the fields that exist in the plotfiles. These should be: [( 'boxlib' , 'gasDensity' ), ( 'boxlib' , 'gasEnergy' ), ( 'boxlib' , 'radEnergy' ), ( 'boxlib' , 'scalar' ), ( 'boxlib' , 'temperature' ), ( 'boxlib' , 'x-GasMomentum' ), ( 'boxlib' , 'x-RadFlux' ), ( 'boxlib' , 'y-GasMomentum' ), ( 'boxlib' , 'y-RadFlux' ), ( 'boxlib' , 'z-GasMomentum' ), ( 'boxlib' , 'z-RadFlux' )] For details, see the yt documentation on reading AMReX data . Tip One of the most useful things to do is to convert the data into a uniform-resolution NumPy array with the covering_grid function. Tip This WarpX script may be useful as a starting point for visualizing a time series of outputs. This script will require some modification to work with Quokka outputs.",
      "title": "yt"
    },
    {
      "location": "postprocessing/#visit",
      "text": "VisIt can read cell-centered output variables from AMReX plotfiles. Currently, there is no support for reading either face-centered variables or particles. (However, by default, cell-centered averages of face-centered variables are included in Quokka plotfiles.) In order to read an individual plotfile, you can select the plt00000/Header file in VisIt's Open dialog box. If you want to read a timeseries of plotfiles, you can create a file with a .visit extension that lists the plt*/Header files, one per line, with the following command: : ls -1 plt*/Header | tee plotfiles.visit Then select plotfiles.visit in VisIt's Open dialog box. Warning There are rendering bugs with unscaled box dimensions. Slices generally work. However, do not expect volume rendering to work when using, e.g. parsec-size boxes with cgs units.",
      "title": "VisIt"
    },
    {
      "location": "running_on_hpc_clusters/",
      "text": "Running on HPC clusters Instructions for running on various HPC clusters are given below. Gadi (NCI Australia) Use the openmpi/4.1.4 module (or newer), and build with gcc/system or gcc/11.1.0 , and use cuda/11.7.0 (or newer). Using VisIt You can use VisIt in client/server mode with the following server-side patch for the launcher script . A host file is provided here . You must change the username, project code, and server-side VisIt path. Setonix (Pawsey) The recommended build procedure on Setonix is: : source scripts/setonix.profile mkdir build; cd build cmake .. -C ../cmake/setonix.cmake make -j16 Then a single-node test job can be run with: : cd .. sbatch scripts/setonix-1node.submit Workaround for interconnect issues If interconnect issues are observed, it is recommended to add the line : export FI_CXI_RX_MATCH_MODE=software to your job scripts.",
      "title": "Running on HPC clusters"
    },
    {
      "location": "running_on_hpc_clusters/#running-on-hpc-clusters",
      "text": "Instructions for running on various HPC clusters are given below.",
      "title": "Running on HPC clusters"
    },
    {
      "location": "running_on_hpc_clusters/#gadi-nci-australia",
      "text": "Use the openmpi/4.1.4 module (or newer), and build with gcc/system or gcc/11.1.0 , and use cuda/11.7.0 (or newer).",
      "title": "Gadi (NCI Australia)"
    },
    {
      "location": "running_on_hpc_clusters/#using-visit",
      "text": "You can use VisIt in client/server mode with the following server-side patch for the launcher script . A host file is provided here . You must change the username, project code, and server-side VisIt path.",
      "title": "Using VisIt"
    },
    {
      "location": "running_on_hpc_clusters/#setonix-pawsey",
      "text": "The recommended build procedure on Setonix is: : source scripts/setonix.profile mkdir build; cd build cmake .. -C ../cmake/setonix.cmake make -j16 Then a single-node test job can be run with: : cd .. sbatch scripts/setonix-1node.submit",
      "title": "Setonix (Pawsey)"
    },
    {
      "location": "running_on_hpc_clusters/#workaround-for-interconnect-issues",
      "text": "If interconnect issues are observed, it is recommended to add the line : export FI_CXI_RX_MATCH_MODE=software to your job scripts.",
      "title": "Workaround for interconnect issues"
    },
    {
      "location": "tests/",
      "text": "Test problems Listed here are the test problems that are included with Quokka. This page is still under construction. Radiative shock test Shu-Osher shock test Slow-moving shock test Matter-radiation temperature equilibrium test Uniform advecting radiation in diffusive limit Advecting radiation pulse test",
      "title": "Test problems"
    },
    {
      "location": "tests/#test-problems",
      "text": "Listed here are the test problems that are included with Quokka. This page is still under construction. Radiative shock test Shu-Osher shock test Slow-moving shock test Matter-radiation temperature equilibrium test Uniform advecting radiation in diffusive limit Advecting radiation pulse test",
      "title": "Test problems"
    },
    {
      "location": "tests/energy_exchange/",
      "text": "Matter-radiation temperature equilibrium test This test problem demonstrates the correct coupled solution of the matter-radiation energy balance equations. We also demonstrate that the equilibrium temperature is incorrect in the reduced speed of light approximation. Parameters The initial energy densities are: \\[\\begin{aligned} E_r = 1.0 \\times 10^{12} \\, \\text{erg} \\, \\text{cm}^{-3} \\\\ E_\\text{gas} = 1.0 \\times 10^2 \\, \\text{erg} \\, \\text{cm}^{-3} \\\\ \\rho = 1.0 \\times 10^{-7} \\, \\text{g} \\, \\text{cm}^{-3} \\end{aligned}\\] We assume a specific heat \\(c_v = \\alpha T^3\\) which enables an analytic solution. We adopt a reduced speed of light with \\(\\hat c = 0.1 c\\) . Solution The exact time-dependent solution for the matter temperature \\(T\\) is: \\[\\begin{aligned} E_0 = E_{\\text{gas}} + \\frac{c}{\\hat c} E_{\\text{rad}} \\\\ \\tilde E_0 = \\frac{E_0}{a_r + \\frac{\\hat c}{c} \\frac{\\alpha}{4}} \\\\ T^4 = \\left( T_{0}^4 - \\frac{\\hat c}{c} \\tilde E_0 \\right) \\, \\exp \\left[ -\\frac{4}{\\alpha} \\left( a_r + \\frac{\\hat c}{c} \\frac{\\alpha}{4} \\right) \\kappa \\rho c t \\right] \\, + \\, \\frac{\\hat c}{c} \\tilde E_0 \\, . \\end{aligned}\\] We show the numerical results below: The radiation temperature and matter temperatures in the reduced speed-of-light approximation, along with the exact solution for the matter temperature. The radiation temperature and matter temperatures, along with the exact solution for the matter temperature.",
      "title": "Matter-radiation temperature equilibrium test"
    },
    {
      "location": "tests/energy_exchange/#matter-radiation-temperature-equilibrium-test",
      "text": "This test problem demonstrates the correct coupled solution of the matter-radiation energy balance equations. We also demonstrate that the equilibrium temperature is incorrect in the reduced speed of light approximation.",
      "title": "Matter-radiation temperature equilibrium test"
    },
    {
      "location": "tests/energy_exchange/#parameters",
      "text": "The initial energy densities are: \\[\\begin{aligned} E_r = 1.0 \\times 10^{12} \\, \\text{erg} \\, \\text{cm}^{-3} \\\\ E_\\text{gas} = 1.0 \\times 10^2 \\, \\text{erg} \\, \\text{cm}^{-3} \\\\ \\rho = 1.0 \\times 10^{-7} \\, \\text{g} \\, \\text{cm}^{-3} \\end{aligned}\\] We assume a specific heat \\(c_v = \\alpha T^3\\) which enables an analytic solution. We adopt a reduced speed of light with \\(\\hat c = 0.1 c\\) .",
      "title": "Parameters"
    },
    {
      "location": "tests/energy_exchange/#solution",
      "text": "The exact time-dependent solution for the matter temperature \\(T\\) is: \\[\\begin{aligned} E_0 = E_{\\text{gas}} + \\frac{c}{\\hat c} E_{\\text{rad}} \\\\ \\tilde E_0 = \\frac{E_0}{a_r + \\frac{\\hat c}{c} \\frac{\\alpha}{4}} \\\\ T^4 = \\left( T_{0}^4 - \\frac{\\hat c}{c} \\tilde E_0 \\right) \\, \\exp \\left[ -\\frac{4}{\\alpha} \\left( a_r + \\frac{\\hat c}{c} \\frac{\\alpha}{4} \\right) \\kappa \\rho c t \\right] \\, + \\, \\frac{\\hat c}{c} \\tilde E_0 \\, . \\end{aligned}\\] We show the numerical results below: The radiation temperature and matter temperatures in the reduced speed-of-light approximation, along with the exact solution for the matter temperature. The radiation temperature and matter temperatures, along with the exact solution for the matter temperature.",
      "title": "Solution"
    },
    {
      "location": "tests/radhydro_pulse/",
      "text": "Advecting radiation pulse test This test demonstrates the code's ability to deal with the relativistic correction source terms that arise from the mixed frame formulation of the RHD moment equations, in a fully-coupled RHD problem. The problems involve the advection of the a pulse of radiation energy in an optically thick ( \\(\\tau \\gg 1\\) ) gas in both static ( \\(\\beta \\tau \\ll 1\\) ) and dynamic ( \\(\\beta \\tau \\gg 1\\) ) diffusion regimes, with a uniform background flow velocity (Krumholz, Klein, McKee, & Bolstad, 2007) 1 . Parameters Initial condition of the problem in static diffusion regime: \\[\\begin{aligned} \\begin{align} T = T_0 + (T_1 - T_0) \\exp \\left( - \\frac{x^2}{2 w^2} \\right), \\\\ w = 24 ~{\\rm cm}, T_0 = 10^7 ~{\\rm K}, T_1 = 2 \\times 10^7 ~{\\rm K} \\\\ \\rho=\\rho_0 \\frac{T_0}{T}+\\frac{a_{\\mathrm{R}} \\mu}{3 k_{\\mathrm{B}}}\\left(\\frac{T_0^4}{T}-T^3\\right) \\\\ \\rho_0 = 1.2 ~{\\rm g~cm^{-3}}, \\mu = 2.33 ~m_{\\rm H} \\\\ \\kappa_P=\\kappa_R=\\kappa = 100 \\mathrm{~cm}^2 \\mathrm{~g}^{-1} \\\\ v = 10 ~{\\rm km~s^{-1}} \\\\ \\tau = \\rho \\kappa w = 3 \\times 10^3, \\beta = v/c = 3 \\times 10^{-5}, \\beta \\tau = 9 \\times 10^{-2} \\end{align} \\end{aligned}\\] The simulation is run till \\(t_{\\rm end} = 2 w/v = 4.8 \\times 10^{-5} ~{\\rm s}\\) . Initial condition of the problem in dynamic diffusion regime: same parameters as in the static diffusion regime except \\[\\begin{aligned} \\begin{align} \\kappa_P=\\kappa_R=\\kappa=1000 \\mathrm{~cm}^2 \\mathrm{~g}^{-1} \\\\ v = 1000 ~{\\rm km~s^{-1}} \\\\ t_{\\rm end} = 2 w/v = 1.2 \\times 10^{-4} ~{\\rm s} \\\\ \\tau = \\rho \\kappa w = 3 \\times 10^4, \\beta = v/c = 3 \\times 10^{-3}, \\beta \\tau = 90 \\end{align} \\end{aligned}\\] Results Static diffusion regime: radhydro_pulse_temperature-static-diffusion radhydro_pulse_density-static-diffusion radhydro_pulse_velocity-static-diffusion Dynamic diffusion regime: radhydro_pulse_temperature-dynamic-diffusion radhydro_pulse_density-dynamic-diffusion radhydro_pulse_velocity-dynamic-diffusion Krumholz, M. R., Klein, R. I., McKee, C. F., & Bolstad, J. (2007). Equations and Algorithms for Mixed-frame Flux-limited Diffusion Radiation Hydrodynamics . The Astrophysical Journal , 667 , 626\u2013643. https://doi.org/10.1086/520791 \u21a9",
      "title": "Advecting radiation pulse test"
    },
    {
      "location": "tests/radhydro_pulse/#advecting-radiation-pulse-test",
      "text": "This test demonstrates the code's ability to deal with the relativistic correction source terms that arise from the mixed frame formulation of the RHD moment equations, in a fully-coupled RHD problem. The problems involve the advection of the a pulse of radiation energy in an optically thick ( \\(\\tau \\gg 1\\) ) gas in both static ( \\(\\beta \\tau \\ll 1\\) ) and dynamic ( \\(\\beta \\tau \\gg 1\\) ) diffusion regimes, with a uniform background flow velocity (Krumholz, Klein, McKee, & Bolstad, 2007) 1 .",
      "title": "Advecting radiation pulse test"
    },
    {
      "location": "tests/radhydro_pulse/#parameters",
      "text": "Initial condition of the problem in static diffusion regime: \\[\\begin{aligned} \\begin{align} T = T_0 + (T_1 - T_0) \\exp \\left( - \\frac{x^2}{2 w^2} \\right), \\\\ w = 24 ~{\\rm cm}, T_0 = 10^7 ~{\\rm K}, T_1 = 2 \\times 10^7 ~{\\rm K} \\\\ \\rho=\\rho_0 \\frac{T_0}{T}+\\frac{a_{\\mathrm{R}} \\mu}{3 k_{\\mathrm{B}}}\\left(\\frac{T_0^4}{T}-T^3\\right) \\\\ \\rho_0 = 1.2 ~{\\rm g~cm^{-3}}, \\mu = 2.33 ~m_{\\rm H} \\\\ \\kappa_P=\\kappa_R=\\kappa = 100 \\mathrm{~cm}^2 \\mathrm{~g}^{-1} \\\\ v = 10 ~{\\rm km~s^{-1}} \\\\ \\tau = \\rho \\kappa w = 3 \\times 10^3, \\beta = v/c = 3 \\times 10^{-5}, \\beta \\tau = 9 \\times 10^{-2} \\end{align} \\end{aligned}\\] The simulation is run till \\(t_{\\rm end} = 2 w/v = 4.8 \\times 10^{-5} ~{\\rm s}\\) . Initial condition of the problem in dynamic diffusion regime: same parameters as in the static diffusion regime except \\[\\begin{aligned} \\begin{align} \\kappa_P=\\kappa_R=\\kappa=1000 \\mathrm{~cm}^2 \\mathrm{~g}^{-1} \\\\ v = 1000 ~{\\rm km~s^{-1}} \\\\ t_{\\rm end} = 2 w/v = 1.2 \\times 10^{-4} ~{\\rm s} \\\\ \\tau = \\rho \\kappa w = 3 \\times 10^4, \\beta = v/c = 3 \\times 10^{-3}, \\beta \\tau = 90 \\end{align} \\end{aligned}\\]",
      "title": "Parameters"
    },
    {
      "location": "tests/radhydro_pulse/#results",
      "text": "Static diffusion regime: radhydro_pulse_temperature-static-diffusion radhydro_pulse_density-static-diffusion radhydro_pulse_velocity-static-diffusion Dynamic diffusion regime: radhydro_pulse_temperature-dynamic-diffusion radhydro_pulse_density-dynamic-diffusion radhydro_pulse_velocity-dynamic-diffusion Krumholz, M. R., Klein, R. I., McKee, C. F., & Bolstad, J. (2007). Equations and Algorithms for Mixed-frame Flux-limited Diffusion Radiation Hydrodynamics . The Astrophysical Journal , 667 , 626\u2013643. https://doi.org/10.1086/520791 \u21a9",
      "title": "Results"
    },
    {
      "location": "tests/radhydro_uniform_adv/",
      "text": "Uniform advecting radiation in diffusive limit In this test, we simulation an advecting uniform gas where radiation and matter are in thermal equilibrium in the co-moving frame. Following the Lorentz tranform, the initial radiation energy and flux in the lab frame to first order in \\(v/c\\) are \\(E_r = a_r T^4\\) and \\(F_r = \\frac{4}{3} v E_r\\) . Parameters \\[\\begin{aligned} \\begin{align} T_0 = 10^7~{\\rm K} \\\\ \\rho_0 = 1.2 ~{\\rm g~cm^{-3}}, \\mu = 2.33 ~m_{\\rm H} \\\\ \\kappa_P=\\kappa_R=100 \\mathrm{~cm}^2 \\mathrm{~g}^{-1} \\\\ v_{x,0} = 10 ~{\\rm km~s^{-1}} \\\\ E_{r,0} = a_r T_0^4 \\\\ F_{x,0} = \\frac{4}{3} v_{x,0} E_{r,0} \\\\ t_{\\rm end} = 4.8 \\times 10^{-5} ~{\\rm s} \\end{align} \\end{aligned}\\] Results With \\(O(\\beta \\tau)\\) terms: The radiation temperature and matter temperatures, along with the exact solution. The matter velocity, along with the exact solution. Without \\(O(\\beta \\tau)\\) terms: The radiation temperature and matter temperatures, along with the exact solution. The matter velocity, along with the exact solution. Physics In the transport equation, both the radiation energy and flux are unchanged because the radiation flux and pressure are uniform. In the matter-radiation exchange step, the source term is zero since the radiation and matter are in equilibrium. Finally, the flux is updated following \\[\\mathbf{F}_{r}^{(t+1)} = \\frac{\\mathbf{F}_{r}^{(t)} + \\Delta t \\left[ \\rho \\kappa_P \\left(\\frac{4 \\pi B}{c}\\right) \\mathbf{v}c + \\rho \\kappa_F (\\mathbf{v} :\\mathsf{P}_r) c \\right] }{1+\\rho \\kappa_{F} {c} \\Delta t}.\\] With \\(F_{r}^{(t)} = 4 v E_{r}^{(t)} / 3\\) , and \\(\\kappa_P=\\kappa_R=\\kappa\\) , we have \\[\\mathbf{F}_{r}^{(t+1)} = \\frac{\\frac{4}{3} v E_r^{(t)} + \\Delta t \\left[ \\rho \\kappa E_r^{(t)} \\mathbf{v}c + \\rho \\kappa \\mathbf{v} (\\frac{1}{3}E_r^{(t)}) c \\right] }{1+\\rho \\kappa {c} \\Delta t} = \\frac{4}{3} v E_r^{(t)} = F_{r}^{(t)}\\] Therefore, \\(F_r\\) remains constant. This demonstrates that the code is invariant under Lorentz transformation. We can also show that, with the \\(O(\\beta \\tau)\\) terms in the matter-radiation exchange step, the space-like component of the radiation four-force vanishes: \\[\\begin{aligned} \\begin{align} -G &= -\\rho \\kappa_F \\frac{\\mathbf{F}_r}{c} + \\rho \\kappa_P\\left(\\frac{4 \\pi B}{c}\\right) \\frac{\\mathbf{v}}{c}+\\rho \\kappa_F \\frac{\\mathbf{v} :\\mathsf{P}_r}{c} \\\\ &= -\\rho \\kappa \\frac{4}{3} E_r v / c + \\rho \\kappa E_r v / c+ \\rho \\kappa \\frac{1}{3} E_r v / c \\\\ &= 0 \\end{align} \\end{aligned}\\]",
      "title": "Uniform advecting radiation in diffusive limit"
    },
    {
      "location": "tests/radhydro_uniform_adv/#uniform-advecting-radiation-in-diffusive-limit",
      "text": "In this test, we simulation an advecting uniform gas where radiation and matter are in thermal equilibrium in the co-moving frame. Following the Lorentz tranform, the initial radiation energy and flux in the lab frame to first order in \\(v/c\\) are \\(E_r = a_r T^4\\) and \\(F_r = \\frac{4}{3} v E_r\\) .",
      "title": "Uniform advecting radiation in diffusive limit"
    },
    {
      "location": "tests/radhydro_uniform_adv/#parameters",
      "text": "\\[\\begin{aligned} \\begin{align} T_0 = 10^7~{\\rm K} \\\\ \\rho_0 = 1.2 ~{\\rm g~cm^{-3}}, \\mu = 2.33 ~m_{\\rm H} \\\\ \\kappa_P=\\kappa_R=100 \\mathrm{~cm}^2 \\mathrm{~g}^{-1} \\\\ v_{x,0} = 10 ~{\\rm km~s^{-1}} \\\\ E_{r,0} = a_r T_0^4 \\\\ F_{x,0} = \\frac{4}{3} v_{x,0} E_{r,0} \\\\ t_{\\rm end} = 4.8 \\times 10^{-5} ~{\\rm s} \\end{align} \\end{aligned}\\]",
      "title": "Parameters"
    },
    {
      "location": "tests/radhydro_uniform_adv/#results",
      "text": "With \\(O(\\beta \\tau)\\) terms: The radiation temperature and matter temperatures, along with the exact solution. The matter velocity, along with the exact solution. Without \\(O(\\beta \\tau)\\) terms: The radiation temperature and matter temperatures, along with the exact solution. The matter velocity, along with the exact solution.",
      "title": "Results"
    },
    {
      "location": "tests/radhydro_uniform_adv/#physics",
      "text": "In the transport equation, both the radiation energy and flux are unchanged because the radiation flux and pressure are uniform. In the matter-radiation exchange step, the source term is zero since the radiation and matter are in equilibrium. Finally, the flux is updated following \\[\\mathbf{F}_{r}^{(t+1)} = \\frac{\\mathbf{F}_{r}^{(t)} + \\Delta t \\left[ \\rho \\kappa_P \\left(\\frac{4 \\pi B}{c}\\right) \\mathbf{v}c + \\rho \\kappa_F (\\mathbf{v} :\\mathsf{P}_r) c \\right] }{1+\\rho \\kappa_{F} {c} \\Delta t}.\\] With \\(F_{r}^{(t)} = 4 v E_{r}^{(t)} / 3\\) , and \\(\\kappa_P=\\kappa_R=\\kappa\\) , we have \\[\\mathbf{F}_{r}^{(t+1)} = \\frac{\\frac{4}{3} v E_r^{(t)} + \\Delta t \\left[ \\rho \\kappa E_r^{(t)} \\mathbf{v}c + \\rho \\kappa \\mathbf{v} (\\frac{1}{3}E_r^{(t)}) c \\right] }{1+\\rho \\kappa {c} \\Delta t} = \\frac{4}{3} v E_r^{(t)} = F_{r}^{(t)}\\] Therefore, \\(F_r\\) remains constant. This demonstrates that the code is invariant under Lorentz transformation. We can also show that, with the \\(O(\\beta \\tau)\\) terms in the matter-radiation exchange step, the space-like component of the radiation four-force vanishes: \\[\\begin{aligned} \\begin{align} -G &= -\\rho \\kappa_F \\frac{\\mathbf{F}_r}{c} + \\rho \\kappa_P\\left(\\frac{4 \\pi B}{c}\\right) \\frac{\\mathbf{v}}{c}+\\rho \\kappa_F \\frac{\\mathbf{v} :\\mathsf{P}_r}{c} \\\\ &= -\\rho \\kappa \\frac{4}{3} E_r v / c + \\rho \\kappa E_r v / c+ \\rho \\kappa \\frac{1}{3} E_r v / c \\\\ &= 0 \\end{align} \\end{aligned}\\]",
      "title": "Physics"
    },
    {
      "location": "tests/radshock/",
      "text": "Radiative shock test This test problem demonstrates the correct coupled solution of the hydrodynamics and radiation moment equations for a subcritical radiative shock. The steady-state solution in the nonequilibrium radiation diffusion approximation is given by a set of coupled ODEs that can be solved to arbitrary precision following the method of (Lowrie & Edwards, 2008) 1 . Parameters The dimensionless shock parameters (Lowrie & Edwards, 2008) 1 are: \\[\\begin{aligned} P_0 = 1.0 \\times 10^{-4} \\\\ \\sigma_a = 1.0 \\times 10^{6} \\\\ \\mathcal{M}_0 = 3.0 \\\\ \\gamma = 5/3 \\\\ \\end{aligned}\\] Following (Skinner, Dolence, Burrows, Radice, & Vartanyan, 2019) 2 , we scale to dimensional values assuming \\[\\begin{aligned} \\mu = m_H \\\\ c_v = \\frac{k_B}{\\mu (\\gamma - 1)} \\, \\text{erg} \\, \\text{g}^{-1} \\, \\text{K}^{-1} \\\\ c_{s,0} = 1.73 \\times 10^{7} \\, \\text{cm} \\, \\text{s}^{-1} \\\\ \\kappa = 577.0 \\, \\text{cm}^{-1} \\\\ \\end{aligned}\\] and obtain the following pre-shock and post-shock states: \\[\\begin{aligned} T_0 = 2.18 \\times 10^6 \\, \\text{K} \\\\ \\rho_0 = 5.69 \\, \\text{g} \\, \\text{cm}^{-3} \\\\ v_0 = 5.19 \\times 10^7 \\, \\text{cm} \\, \\text{s}^{-1} \\\\ T_1 = 7.98\\times 10^6 \\, \\text{K} \\\\ \\rho_1 = 17.1 \\, \\text{g} \\, \\text{cm}^{-3} \\\\ v_1 = 1.73 \\times 10^7 \\, \\text{cm} \\, \\text{s}^{-1} \\, . \\end{aligned}\\] We adopt a reduced speed of light (as used in (Skinner, Dolence, Burrows, Radice, & Vartanyan, 2019) 2 ) \\[\\hat c = 10 (v_0 + c_{s,0}) \\, .\\] Solution Since the solution is given assuming radiation diffusion, we set the Eddington factor (as used in the Riemann solver for the radiation moment equations) to a constant value of \\(1/3\\) everywhere. We use the RK2 integrator with a CFL number of 0.2 and a mesh of 256 equally-spaced zones. After 3 shock crossing times, we obtain a solution for the radiation temperature and matter temperature that agrees to better than 0.5% (in relative L1 norm) with the steady-state ODE solution to the radiation hydrodynamics equations: The radiation temperature is shown in the black solid and dashed lines, with the dashed line showing the semi-analytic solution. The material temperature is shown in the red lines, with the semi-analytic solution shown with the dashed line. Lowrie, R. B., & Edwards, J. D. (2008). Radiative shock solutions with grey nonequilibrium diffusion . Shock Waves , 18 (2), 129\u2013143. https://doi.org/10.1007/s00193-008-0143-0 \u21a9 \u21a9 Skinner, M. A., Dolence, J. C., Burrows, A., Radice, D., & Vartanyan, D. (2019). FORNAX: A Flexible Code for Multiphysics Astrophysical Simulations . 241 (1), 7. https://doi.org/10.3847/1538-4365/ab007f \u21a9 \u21a9",
      "title": "Radiative shock test"
    },
    {
      "location": "tests/radshock/#radiative-shock-test",
      "text": "This test problem demonstrates the correct coupled solution of the hydrodynamics and radiation moment equations for a subcritical radiative shock. The steady-state solution in the nonequilibrium radiation diffusion approximation is given by a set of coupled ODEs that can be solved to arbitrary precision following the method of (Lowrie & Edwards, 2008) 1 .",
      "title": "Radiative shock test"
    },
    {
      "location": "tests/radshock/#parameters",
      "text": "The dimensionless shock parameters (Lowrie & Edwards, 2008) 1 are: \\[\\begin{aligned} P_0 = 1.0 \\times 10^{-4} \\\\ \\sigma_a = 1.0 \\times 10^{6} \\\\ \\mathcal{M}_0 = 3.0 \\\\ \\gamma = 5/3 \\\\ \\end{aligned}\\] Following (Skinner, Dolence, Burrows, Radice, & Vartanyan, 2019) 2 , we scale to dimensional values assuming \\[\\begin{aligned} \\mu = m_H \\\\ c_v = \\frac{k_B}{\\mu (\\gamma - 1)} \\, \\text{erg} \\, \\text{g}^{-1} \\, \\text{K}^{-1} \\\\ c_{s,0} = 1.73 \\times 10^{7} \\, \\text{cm} \\, \\text{s}^{-1} \\\\ \\kappa = 577.0 \\, \\text{cm}^{-1} \\\\ \\end{aligned}\\] and obtain the following pre-shock and post-shock states: \\[\\begin{aligned} T_0 = 2.18 \\times 10^6 \\, \\text{K} \\\\ \\rho_0 = 5.69 \\, \\text{g} \\, \\text{cm}^{-3} \\\\ v_0 = 5.19 \\times 10^7 \\, \\text{cm} \\, \\text{s}^{-1} \\\\ T_1 = 7.98\\times 10^6 \\, \\text{K} \\\\ \\rho_1 = 17.1 \\, \\text{g} \\, \\text{cm}^{-3} \\\\ v_1 = 1.73 \\times 10^7 \\, \\text{cm} \\, \\text{s}^{-1} \\, . \\end{aligned}\\] We adopt a reduced speed of light (as used in (Skinner, Dolence, Burrows, Radice, & Vartanyan, 2019) 2 ) \\[\\hat c = 10 (v_0 + c_{s,0}) \\, .\\]",
      "title": "Parameters"
    },
    {
      "location": "tests/radshock/#solution",
      "text": "Since the solution is given assuming radiation diffusion, we set the Eddington factor (as used in the Riemann solver for the radiation moment equations) to a constant value of \\(1/3\\) everywhere. We use the RK2 integrator with a CFL number of 0.2 and a mesh of 256 equally-spaced zones. After 3 shock crossing times, we obtain a solution for the radiation temperature and matter temperature that agrees to better than 0.5% (in relative L1 norm) with the steady-state ODE solution to the radiation hydrodynamics equations: The radiation temperature is shown in the black solid and dashed lines, with the dashed line showing the semi-analytic solution. The material temperature is shown in the red lines, with the semi-analytic solution shown with the dashed line. Lowrie, R. B., & Edwards, J. D. (2008). Radiative shock solutions with grey nonequilibrium diffusion . Shock Waves , 18 (2), 129\u2013143. https://doi.org/10.1007/s00193-008-0143-0 \u21a9 \u21a9 Skinner, M. A., Dolence, J. C., Burrows, A., Radice, D., & Vartanyan, D. (2019). FORNAX: A Flexible Code for Multiphysics Astrophysical Simulations . 241 (1), 7. https://doi.org/10.3847/1538-4365/ab007f \u21a9 \u21a9",
      "title": "Solution"
    },
    {
      "location": "tests/shu_osher/",
      "text": "Shu-Osher shock test This test problem demonstrates the ability of the code to resolve fine features and shocks simultaneously. The resolution of our code is comparable to the ENO-RF-3 scheme shown in Figure 14b of (Shu & Osher, 1989) 1 . Parameters The left- ( \\(x < 1.0\\) ) and right-side ( \\(x \\ge 1.0\\) ) initial conditions are: \\[\\begin{aligned} \\rho_0 = 3.857143 \\\\ v_{x,0} = 2.629369 \\\\ P_0 = 10.33333 \\\\ \\rho_1 = 1 + 0.2 \\sin(5 x) \\\\ v_{x,1} = 0 \\\\ P_1 = 1 \\end{aligned}\\] Solution We use the RK2 integrator with a fixed timestep of \\(10^{-4}\\) and a mesh of 400 equally-spaced zones, evolving until time \\(t=1.8\\) . There are some subtle stair-step artifacts similar to those seen in the sawtooth linear advection test, but these converge away as the spatial resolution is increased. These artefacts can be eliminated by projecting into characteristic waves and reconstructing the interface states in the characteristic variables, as done in \u00a74 of (Shu & Osher, 1989) 1 . The reference solution is computed using Athena++ with PPM reconstruction in the characteristic variables on a grid of 1600 zones. The density is shown as the solid blue line. There is no exact solution for this problem. Shu, C.-W., & Osher, S. (1989). Efficient Implementation of Essentially Non-oscillatory Shock-Capturing Schemes, II . Journal of Computational Physics , 83 (1), 32\u201378. https://doi.org/10.1016/0021-9991(89)90222-2 \u21a9 \u21a9",
      "title": "Shu-Osher shock test"
    },
    {
      "location": "tests/shu_osher/#shu-osher-shock-test",
      "text": "This test problem demonstrates the ability of the code to resolve fine features and shocks simultaneously. The resolution of our code is comparable to the ENO-RF-3 scheme shown in Figure 14b of (Shu & Osher, 1989) 1 .",
      "title": "Shu-Osher shock test"
    },
    {
      "location": "tests/shu_osher/#parameters",
      "text": "The left- ( \\(x < 1.0\\) ) and right-side ( \\(x \\ge 1.0\\) ) initial conditions are: \\[\\begin{aligned} \\rho_0 = 3.857143 \\\\ v_{x,0} = 2.629369 \\\\ P_0 = 10.33333 \\\\ \\rho_1 = 1 + 0.2 \\sin(5 x) \\\\ v_{x,1} = 0 \\\\ P_1 = 1 \\end{aligned}\\]",
      "title": "Parameters"
    },
    {
      "location": "tests/shu_osher/#solution",
      "text": "We use the RK2 integrator with a fixed timestep of \\(10^{-4}\\) and a mesh of 400 equally-spaced zones, evolving until time \\(t=1.8\\) . There are some subtle stair-step artifacts similar to those seen in the sawtooth linear advection test, but these converge away as the spatial resolution is increased. These artefacts can be eliminated by projecting into characteristic waves and reconstructing the interface states in the characteristic variables, as done in \u00a74 of (Shu & Osher, 1989) 1 . The reference solution is computed using Athena++ with PPM reconstruction in the characteristic variables on a grid of 1600 zones. The density is shown as the solid blue line. There is no exact solution for this problem. Shu, C.-W., & Osher, S. (1989). Efficient Implementation of Essentially Non-oscillatory Shock-Capturing Schemes, II . Journal of Computational Physics , 83 (1), 32\u201378. https://doi.org/10.1016/0021-9991(89)90222-2 \u21a9 \u21a9",
      "title": "Solution"
    },
    {
      "location": "tests/sms/",
      "text": "Slow-moving shock test This test problem demonstrates the extent to which post-shock oscillations are controlled in slowly-moving shocks. This effect can be exhibited in all Godunov codes, even with first-order methods, for sufficiently slow-moving shocks across the computational grid (Jin & Liu, 1996) 1 . The shock flattening method of (Colella & Woodward, 1984) 2 (implemented in our code in modified form) reduces the oscillations, but does not completely suppress them. Adding artificial viscosity according to the method of (Colella & Woodward, 1984) 2 , even to the level of smoothing the contact discontinuity by 5-10 cells, does not cure the problem. Parameters The left- and right-side initial conditions are (Quirk, 1994) 3 : \\[\\begin{aligned} \\rho_0 = 3.86 \\\\ v_{x,0} = -0.81 \\\\ P_0 = 10.3334 \\\\ \\rho_0 = 1.0 \\\\ v_{x,0} = -3.44 \\\\ P_0 = 1.0 \\end{aligned}\\] The shock moves to the right with speed \\(s = 0.1096\\) . Solution We use the RK2 integrator with a fixed timestep of \\(10^{-3}\\) and a mesh of 100 equally-spaced cells. The contact discontinuity is initially placed at \\(x=0.5\\) . The density is shown as the solid blue line. The exact solution is the solid orange line. Jin, S., & Liu, J.-G. (1996). The Effects of Numerical Viscosities. I. Slowly Moving Shocks . Journal of Computational Physics , 126 (2), 373\u2013389. https://doi.org/10.1006/jcph.1996.0144 \u21a9 Colella, P., & Woodward, P. R. (1984). The Piecewise Parabolic Method (PPM) for Gas-Dynamical Simulations . Journal of Computational Physics , 54 , 174\u2013201. https://doi.org/10.1016/0021-9991(84)90143-8 \u21a9 \u21a9 Quirk, J. J. (1994). A contribution to the great Riemann solver debate . International Journal for Numerical Methods in Fluids , 18 (6), 555\u2013574. https://doi.org/10.1002/fld.1650180603 \u21a9",
      "title": "Slow-moving shock test"
    },
    {
      "location": "tests/sms/#slow-moving-shock-test",
      "text": "This test problem demonstrates the extent to which post-shock oscillations are controlled in slowly-moving shocks. This effect can be exhibited in all Godunov codes, even with first-order methods, for sufficiently slow-moving shocks across the computational grid (Jin & Liu, 1996) 1 . The shock flattening method of (Colella & Woodward, 1984) 2 (implemented in our code in modified form) reduces the oscillations, but does not completely suppress them. Adding artificial viscosity according to the method of (Colella & Woodward, 1984) 2 , even to the level of smoothing the contact discontinuity by 5-10 cells, does not cure the problem.",
      "title": "Slow-moving shock test"
    },
    {
      "location": "tests/sms/#parameters",
      "text": "The left- and right-side initial conditions are (Quirk, 1994) 3 : \\[\\begin{aligned} \\rho_0 = 3.86 \\\\ v_{x,0} = -0.81 \\\\ P_0 = 10.3334 \\\\ \\rho_0 = 1.0 \\\\ v_{x,0} = -3.44 \\\\ P_0 = 1.0 \\end{aligned}\\] The shock moves to the right with speed \\(s = 0.1096\\) .",
      "title": "Parameters"
    },
    {
      "location": "tests/sms/#solution",
      "text": "We use the RK2 integrator with a fixed timestep of \\(10^{-3}\\) and a mesh of 100 equally-spaced cells. The contact discontinuity is initially placed at \\(x=0.5\\) . The density is shown as the solid blue line. The exact solution is the solid orange line. Jin, S., & Liu, J.-G. (1996). The Effects of Numerical Viscosities. I. Slowly Moving Shocks . Journal of Computational Physics , 126 (2), 373\u2013389. https://doi.org/10.1006/jcph.1996.0144 \u21a9 Colella, P., & Woodward, P. R. (1984). The Piecewise Parabolic Method (PPM) for Gas-Dynamical Simulations . Journal of Computational Physics , 54 , 174\u2013201. https://doi.org/10.1016/0021-9991(84)90143-8 \u21a9 \u21a9 Quirk, J. J. (1994). A contribution to the great Riemann solver debate . International Journal for Numerical Methods in Fluids , 18 (6), 555\u2013574. https://doi.org/10.1002/fld.1650180603 \u21a9",
      "title": "Solution"
    }
  ]
}